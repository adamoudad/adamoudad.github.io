<!doctype html>
<html lang="en-us">
  <head>
    <title>Vectorizing text: Word2Vec // Adam Oudad</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.57.2" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="Adam Oudad" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://adamoudad.github.io/css/main.min.59023e5fd38d6ecb0e1dfbb295077c3c67e00e3b9eb3feaf34b5a5e6b332897a.css" />

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-150494000-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Vectorizing text: Word2Vec"/>
<meta name="twitter:description" content="The idea with greatest impact in Word2Vec1 2 was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics vec(&#39;Rome&#39;) ≈ vec(&#39;Paris&#39;) - vec(&#39;France&#39;) &#43; vec(&#39;Italy&#39;) vec(&#39;Queen&#39;) ≈ vec(&#39;King&#39;) - vec(&#39;Man&#39;) &#43; vec(&#39;Woman&#39;)   Awesome ! How could such results be achieved ? They came from the following assumption   The meaning of a word can be inferred by the company it keeps   Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context."/>

    <meta property="og:title" content="Vectorizing text: Word2Vec" />
<meta property="og:description" content="The idea with greatest impact in Word2Vec1 2 was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics vec(&#39;Rome&#39;) ≈ vec(&#39;Paris&#39;) - vec(&#39;France&#39;) &#43; vec(&#39;Italy&#39;) vec(&#39;Queen&#39;) ≈ vec(&#39;King&#39;) - vec(&#39;Man&#39;) &#43; vec(&#39;Woman&#39;)   Awesome ! How could such results be achieved ? They came from the following assumption   The meaning of a word can be inferred by the company it keeps   Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adamoudad.github.io/post/word2vec/" />
<meta property="article:published_time" content="2019-11-27T03:49:04+09:00" />
<meta property="article:modified_time" content="2019-11-27T03:49:04+09:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://adamoudad.github.io/"><img class="app-header-avatar" src="/avatar.jpg" alt="Adam Oudad" /></a>
      <h1>Adam Oudad</h1>
      <p>(Machine) Learning log</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/adamoudad"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github">
  <title>github</title>
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
</svg></a>
        
          <a target="_blank" href="https://www.linkedin.com/in/adam-oudad-9436b866/"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-linkedin">
  <title>linkedin</title>
  <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">Vectorizing text: Word2Vec</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 27, 2019
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          3 min read
        </div><div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag">
  <title>tag</title>
  <path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line>
</svg>
          <a class="tag" href="https://adamoudad.github.io/tags/nlp/">nlp</a><a class="tag" href="https://adamoudad.github.io/tags/embedding/">embedding</a></div></div>
    </header>
    <div class="post-content">
      
<p>
The idea with greatest impact in Word2Vec<sup class="footnote-reference"><a id="footnote-reference-1" href="#footnote-1">1</a></sup> <sup class="footnote-reference"><a id="footnote-reference-2" href="#footnote-2">2</a></sup> was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics
</p>
<pre class="example">
vec('Rome') ≈ vec('Paris') - vec('France') + vec('Italy')
vec('Queen') ≈ vec('King') - vec('Man') + vec('Woman')
</pre>
<p>
Awesome !
How could such results be achieved ? They came from the following assumption
</p>
<blockquote>
<p>
The meaning of a word can be inferred by the company it keeps
</p>
</blockquote>
<p>
Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context.
</p>
<p>
The above amazing example of word vectors having semantic relationship when performing arithmetic is actually &#34;too good to be true&#34; for Word2Vec, as you might not obtain the expected relationship if you compute it with the pretrained models. The original paper by the Google team in 2014<sup class="footnote-reference"><a id="footnote-reference-3" href="#footnote-3">3</a></sup> however shows the following relationships are obtained when projecting with Principal Component Analysis.
<figure>
    <img src="word2vec-pca.png" width="100%"/> 
</figure>

</p>
<h1 id="headline-1">
Continuous Bag-of-words
</h1>
<p>
<figure>
    <img src="https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png"
         alt="Word2vec model. Source: wildml.com" width="100%"/> <figcaption>
            <p>Word2vec model. Source: wildml.com</p>
        </figcaption>
</figure>

The Word2Vec model is used to predict a word, given its context.
</p>
<h2 id="headline-2">
Words are one-hot vectors
</h2>
<p>
Raw words are represented using one-hot encoding. That is, if your vocabulary has \(d\) words, the ith word of the vocabulary is represented by \(w\), a binary vector in \(\mathbb{R}^d\) with the ith bit activated, all other bits being off.
Say &#34;cat, dog, eat&#34; are all words in your vocabulary. In this order, we will encode &#34;cat&#34; with \([1, 0, 0]\), &#34;dog&#34; with \([0, 1, 0]\), &#34;eat&#34; with \([0, 0, 1]\).
</p>
<h2 id="headline-3">
A word&#39;s context is a continuous bag of words
</h2>
<p>
For representing the context of a word, Continuous Bag-of-words (or CBOW) representation is used. In this representation, each words is encoded in a continuous representation, then we take the average representation.
\[
h(c_1,c_2,c_3) = \frac{1}{3}(W_i(c_1 + c_2 + c_3)) 
\]
\(H\) computes the continous representation of context vector formed by three words \(c_1, c_2, c_3\). \(W_i\) serves in computing a word embedding for context words.
From this continuous representation, we derive the target word by decoding it into a word vector \(o = W_oh(c_1, c_2, c_3)\).
</p>
<p>
We see such model can be trained in an unsupervised way, taking four consecutive words at a time, and for example using the second word as target, and the remaining three other words as context of the target word.
</p>
<h2 id="headline-4">
CBOW reversed is Skip-gram 
</h2>
<p>
Skip-gram reverses CBOW model, so that we try to find context words given an input word. To train such network using backpropagation, we sum the output layers, so that only one vector remains.
</p>
<p>
CBOW and Skip-gram both end up computing a hidden representation of words, given a large amount of texts. The original paper by Mikolov et al. used Skip-gram, as well as other advanced techniques, such as negative sampling, which are out of the scope of this article.
</p>
<h1 id="headline-5">
Word2vec in action
</h1>
<h2 id="headline-6">
Gensim
</h2>
<p>
Let&#39;s use <em>gensim</em> library that implements word2vec model. We download english Wikipedia corpus using the convinent <a href="https://github.com/RaRe-Technologies/gensim-data">gensim-data api</a>.
</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  <span style="color:#f92672">from</span> gensim.models.word2vec <span style="color:#f92672">import</span> Word2Vec
  <span style="color:#f92672">import</span> gensim.downloader <span style="color:#f92672">as</span> api

  corpus <span style="color:#f92672">=</span> api<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;text8&#39;</span>)  <span style="color:#75715e"># download english Wikipedia corpus</span>
  model <span style="color:#f92672">=</span> Word2vec(corpus)
  model<span style="color:#f92672">.</span>most_similar(<span style="color:#e6db74">&#34;cat&#34;</span>)</code></pre></div>
</div>
<p>
We obtain as result
</p>
<pre class="example">
  [('dog', 0.8296105861663818),
  ('bee', 0.7931321859359741),
  ('panda', 0.7909268736839294),
  ('hamster', 0.7631657719612122),
  ('bird', 0.7588882446289062),
  ('blonde', 0.7569558620452881),
  ('pig', 0.7523074746131897),
  ('goat', 0.7510213851928711),
  ('ass', 0.7371785640716553),
  ('sheep', 0.7347163558006287)]
</pre>
<h2 id="headline-7">
spaCy
</h2>
<p>
<a href="https://spacy.io/">spaCy</a> is an advanced library for &#34;industrial-strength NLP&#34;.
After installing spaCy, download the pretrained model 
</p>
<div class="src src-bash">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">python -m download en_core_web_lg</code></pre></div>
</div>
<p>
Then
</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  <span style="color:#f92672">from</span> itertools <span style="color:#f92672">import</span> product
  <span style="color:#f92672">import</span> spacy

  nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;en_core_web_md&#34;</span>)
  tokens <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">&#34;dog cat banana afskfsd&#34;</span>)

  <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;# Info on each token dog, cat, banana, and afskfsd&#34;</span>)
  <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens:
      <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>has_vector, token<span style="color:#f92672">.</span>vector_norm, token<span style="color:#f92672">.</span>is_oov)

  <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;# Similarity of dog, cat and banana&#34;</span>)
  <span style="color:#66d9ef">for</span> t1, t2 <span style="color:#f92672">in</span> product(tokens[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], tokens[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]):
      <span style="color:#66d9ef">print</span>(t1<span style="color:#f92672">.</span>similarity(t2))</code></pre></div>
</div>
<p>
Source: <a href="https://spacy.io/usage/vectors-similarity">https://spacy.io/usage/vectors-similarity</a>
</p>
<h1 id="headline-8">
Footnotes
</h1>
<div class="footnotes">
<hr class="footnotes-separatator">
<div class="footnote-definitions">
<div class="footnote-definition">
<sup id="footnote-1"><a href="#footnote-reference-1">1</a></sup>
<div class="footnote-body">
<p>
<a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>, 2013
</p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-2"><a href="#footnote-reference-2">2</a></sup>
<div class="footnote-body">
<p>
<a href="https://arxiv.org/abs/1402.3722">Word2vec Explained: deriving Mikolov et al.&#39;s negative-sampling word-embedding method</a>, 2014
</p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-3"><a href="#footnote-reference-3">3</a></sup>
<div class="footnote-body">
<p>
<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, 2014
</p>
</div>
</div>
</div>
</div>

    </div>
    <div class="post-footer">
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "adamoudad-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
  </article>

    </main>
    </body>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

</html>

