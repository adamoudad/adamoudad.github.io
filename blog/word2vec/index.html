<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<title>Vectorizing text: Word2Vec - Adam Oudad</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="generator" content="Hugo 0.67.0" /><meta itemprop="name" content="Vectorizing text: Word2Vec">
<meta itemprop="description" content="The idea with greatest impact in Word2Vec1 2 was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics vec(&#39;Rome&#39;) ≈ vec(&#39;Paris&#39;) - vec(&#39;France&#39;) &#43; vec(&#39;Italy&#39;) vec(&#39;Queen&#39;) ≈ vec(&#39;King&#39;) - vec(&#39;Man&#39;) &#43; vec(&#39;Woman&#39;)   Awesome ! How could such results be achieved ? They came from the following assumption   The meaning of a word can be inferred by the company it keeps   Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context.">
<meta itemprop="datePublished" content="2019-11-27T03:49:04&#43;09:00" />
<meta itemprop="dateModified" content="2019-11-27T03:49:04&#43;09:00" />
<meta itemprop="wordCount" content="622">



<meta itemprop="keywords" content="nlp,embedding," /><meta property="og:title" content="Vectorizing text: Word2Vec" />
<meta property="og:description" content="The idea with greatest impact in Word2Vec1 2 was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics vec(&#39;Rome&#39;) ≈ vec(&#39;Paris&#39;) - vec(&#39;France&#39;) &#43; vec(&#39;Italy&#39;) vec(&#39;Queen&#39;) ≈ vec(&#39;King&#39;) - vec(&#39;Man&#39;) &#43; vec(&#39;Woman&#39;)   Awesome ! How could such results be achieved ? They came from the following assumption   The meaning of a word can be inferred by the company it keeps   Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adamoudad.github.io/blog/word2vec/" />
<meta property="article:published_time" content="2019-11-27T03:49:04+09:00" />
<meta property="article:modified_time" content="2019-11-27T03:49:04+09:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Vectorizing text: Word2Vec"/>
<meta name="twitter:description" content="The idea with greatest impact in Word2Vec1 2 was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics vec(&#39;Rome&#39;) ≈ vec(&#39;Paris&#39;) - vec(&#39;France&#39;) &#43; vec(&#39;Italy&#39;) vec(&#39;Queen&#39;) ≈ vec(&#39;King&#39;) - vec(&#39;Man&#39;) &#43; vec(&#39;Woman&#39;)   Awesome ! How could such results be achieved ? They came from the following assumption   The meaning of a word can be inferred by the company it keeps   Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context."/>
<meta name="twitter:site" content="@OudadAdam"/>
<link rel="stylesheet" href="/css/bundle.min.a86839252b4aa4853111ceba45af5da1d6c3dc66a6a3b6854750bb2df61dfe37.css" integrity="sha256-qGg5JStKpIUxEc66Ra9dodbD3Gamo7aFR1C7LfYd/jc=">
        <link rel="stylesheet" href="/css/add-on.css">
</head>

  <body>
    
<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/">
        
          
            Blog
          
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu">
      
        
          
          
            <a href="/" class="link"><i class='fa fa-home'></i> Home</a>
          
        
      
        
          
          
            <a href="/about/" class="link"><i class='far fa-id-card'></i> About</a>
          
        
      
        
          
          
            <a href="/blog/" class="link"><i class='far fa-newspaper'></i> Blog</a>
          
        
      
        
          
          
            <a href="/categories/" class="link"><i class='fas fa-sitemap'></i> Categories</a>
          
        
      
        
          
          
            <a href="/contact/" class="link"><i class='far fa-envelope'></i> Contact</a>
          
        
      
      <a href="#share-menu" class="share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      

    </menu>
    

    <a href="#share-menu" class="share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="lang-menu" class="flyout-menu">
  <a href="#" lang="en" class="link active">English (en)</a>
  
    
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Vectorizing%20text%3a%20Word2Vec&amp;url=https%3a%2f%2fadamoudad.github.io%2fblog%2fword2vec%2f" target="_blank" rel="noopener" class="share-btn twitter">
        <i class="fab fa-twitter"></i><p>&nbsp;Twitter</p>
      </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fadamoudad.github.io%2fblog%2fword2vec%2f&amp;title=Vectorizing%20text%3a%20Word2Vec" target="_blank" rel="noopener" class="share-btn linkedin">
            <i class="fab fa-linkedin"></i><p>&nbsp;LinkedIn</p>
          </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/"><img src="/avatar.jpg" class="circle" width="150" alt="Adam Oudad" /></a>
  <header>
    <h1>Adam Oudad</h1>
  </header>
  <main>
    <p>(Machine) Learning log.</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/adamoudad" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>

<li><a href="//stackoverflow.com/users/7013114/adam-oudad" target="_blank" rel="noopener" title="Stack Overflow" class="fab fa-stack-overflow"></a></li>








<li><a href="//medium.com/@adam.oudad" target="_blank" rel="noopener" title="Medium" class="fab fa-medium"></a></li>
<li><a href="//linkedin.com/in/adam-oudad-9436b866" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>















<li><a href="//twitter.com/OudadAdam" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>







<li><a href="//researchgate.net/profile/Adam_Oudad" target="_blank" rel="noopener" title="Research Gate"><i class="ai ai-researchgate"></i></a></li>




      </ul>
    </footer>
  
</section>

      <main id="site-main">
        <article class="post">
            
            <header>
  <div class="title">
    
      <h1><a href="/blog/word2vec/">Vectorizing text: Word2Vec</a></h1>
      
    
    
  </div>
</header>

            <div id="content">
                
<p>
The idea with greatest impact in Word2Vec<sup class="footnote-reference"><a id="footnote-reference-1" href="#footnote-1">1</a></sup> <sup class="footnote-reference"><a id="footnote-reference-2" href="#footnote-2">2</a></sup> was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics
</p>
<pre class="example">
vec('Rome') ≈ vec('Paris') - vec('France') + vec('Italy')
vec('Queen') ≈ vec('King') - vec('Man') + vec('Woman')
</pre>
<p>
Awesome !
  How could such results be achieved ? They came from the following assumption
</p>
<blockquote>
<p>
The meaning of a word can be inferred by the company it keeps
</p>
</blockquote>
<p>
Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context.
</p>
<p>
  The above amazing example of word vectors having semantic relationship when performing arithmetic is actually &#34;too good to be true&#34; for Word2Vec, as you might not obtain the expected relationship if you compute it with the pretrained models. The original paper by the Google team in 2014<sup class="footnote-reference"><a id="footnote-reference-3" href="#footnote-3">3</a></sup> however shows the following relationships are obtained when projecting with Principal Component Analysis.
<figure>
    <img src="word2vec-pca.png" width="100%"/> 
</figure>

</p>
<h2 id="headline-1">
Continuous Bag-of-words
</h2>
<p>
<figure>
    <img src="https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png"
         alt="Word2vec model. Source: wildml.com" width="100%"/> <figcaption>
            <p>Word2vec model. Source: wildml.com</p>
        </figcaption>
</figure>

  The Word2Vec model is used to predict a word, given its context.
</p>
<h3 id="headline-2">
Words are one-hot vectors
</h3>
<p>
Raw words are represented using one-hot encoding. That is, if your vocabulary has \(d\) words, the ith word of the vocabulary is represented by \(w\), a binary vector in \(\mathbb{R}^d\) with the ith bit activated, all other bits being off.
   Say &#34;cat, dog, eat&#34; are all words in your vocabulary. In this order, we will encode &#34;cat&#34; with \([1, 0, 0]\), &#34;dog&#34; with \([0, 1, 0]\), &#34;eat&#34; with \([0, 0, 1]\).
</p>
<h3 id="headline-3">
A word&#39;s context is a continuous bag of words
</h3>
<p>
For representing the context of a word, Continuous Bag-of-words (or CBOW) representation is used. In this representation, each words is encoded in a continuous representation, then we take the average representation.
   \[
   h(c_1,c_2,c_3) = \frac{1}{3}(W_i(c_1 + c_2 + c_3)) 
   \]
   \(H\) computes the continous representation of context vector formed by three words \(c_1, c_2, c_3\). \(W_i\) serves in computing a word embedding for context words.
   From this continuous representation, we derive the target word by decoding it into a word vector \(o = W_oh(c_1, c_2, c_3)\).
</p>
<p>
   We see such model can be trained in an unsupervised way, taking four consecutive words at a time, and for example using the second word as target, and the remaining three other words as context of the target word.
</p>
<h3 id="headline-4">
CBOW reversed is Skip-gram 
</h3>
<p>
Skip-gram reverses CBOW model, so that we try to find context words given an input word. To train such network using backpropagation, we sum the output layers, so that only one vector remains.
</p>
<p>
  CBOW and Skip-gram both end up computing a hidden representation of words, given a large amount of texts. The original paper by Mikolov et al. used Skip-gram, as well as other advanced techniques, such as negative sampling, which are out of the scope of this article.
</p>
<h2 id="headline-5">
Word2vec in action
</h2>
<h3 id="headline-6">
Gensim
</h3>
<p>
Let&#39;s use <em>gensim</em> library that implements word2vec model. We download english Wikipedia corpus using the convinent <a href="https://github.com/RaRe-Technologies/gensim-data">gensim-data api</a>.
</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  <span style="color:#f92672">from</span> gensim.models.word2vec <span style="color:#f92672">import</span> Word2Vec
  <span style="color:#f92672">import</span> gensim.downloader <span style="color:#f92672">as</span> api

  corpus <span style="color:#f92672">=</span> api<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;text8&#39;</span>)  <span style="color:#75715e"># download english Wikipedia corpus</span>
  model <span style="color:#f92672">=</span> Word2vec(corpus)
  model<span style="color:#f92672">.</span>most_similar(<span style="color:#e6db74">&#34;cat&#34;</span>)</code></pre></div>
</div>
<p>
We obtain as result
</p>
<pre class="example">
  [('dog', 0.8296105861663818),
  ('bee', 0.7931321859359741),
  ('panda', 0.7909268736839294),
  ('hamster', 0.7631657719612122),
  ('bird', 0.7588882446289062),
  ('blonde', 0.7569558620452881),
  ('pig', 0.7523074746131897),
  ('goat', 0.7510213851928711),
  ('ass', 0.7371785640716553),
  ('sheep', 0.7347163558006287)]
</pre>
<h3 id="headline-7">
spaCy
</h3>
<p>
<a href="https://spacy.io/">spaCy</a> is an advanced library for &#34;industrial-strength NLP&#34;.
   After installing spaCy, download the pretrained model 
</p>
<div class="src src-bash">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">python -m download en_core_web_lg</code></pre></div>
</div>
<p>
Then
</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  <span style="color:#f92672">from</span> itertools <span style="color:#f92672">import</span> product
  <span style="color:#f92672">import</span> spacy

  nlp <span style="color:#f92672">=</span> spacy<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;en_core_web_md&#34;</span>)
  tokens <span style="color:#f92672">=</span> nlp(<span style="color:#e6db74">&#34;dog cat banana afskfsd&#34;</span>)

  <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;# Info on each token dog, cat, banana, and afskfsd&#34;</span>)
  <span style="color:#66d9ef">for</span> token <span style="color:#f92672">in</span> tokens:
      <span style="color:#66d9ef">print</span>(token<span style="color:#f92672">.</span>text, token<span style="color:#f92672">.</span>has_vector, token<span style="color:#f92672">.</span>vector_norm, token<span style="color:#f92672">.</span>is_oov)

  <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;# Similarity of dog, cat and banana&#34;</span>)
  <span style="color:#66d9ef">for</span> t1, t2 <span style="color:#f92672">in</span> product(tokens[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], tokens[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]):
      <span style="color:#66d9ef">print</span>(t1<span style="color:#f92672">.</span>similarity(t2))</code></pre></div>
</div>
<p>
Source: <a href="https://spacy.io/usage/vectors-similarity">https://spacy.io/usage/vectors-similarity</a>
</p>
<h2 id="headline-8">
Footnotes
</h2>
<div class="footnotes">
<hr class="footnotes-separatator">
<div class="footnote-definitions">
<div class="footnote-definition">
<sup id="footnote-1"><a href="#footnote-reference-1">1</a></sup>
<div class="footnote-body">
<p>
<a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>, 2013
</p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-2"><a href="#footnote-reference-2">2</a></sup>
<div class="footnote-body">
<p>
<a href="https://arxiv.org/abs/1402.3722">Word2vec Explained: deriving Mikolov et al.&#39;s negative-sampling word-embedding method</a>, 2014
</p>
</div>
</div>
<div class="footnote-definition">
<sup id="footnote-3"><a href="#footnote-reference-3">3</a></sup>
<div class="footnote-body">
<p>
<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, 2014
</p>
</div>
</div>
</div>
</div>

            </div>
        </article>
      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent posts</h1>
      </header>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/blog/pythonic/">Tips to write pythonic</a></h1>
          <time class="published" datetime="">February 2, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/blog/japanese/bring-or-take/">Bring and take concepts in languages</a></h1>
          <time class="published" datetime="">January 24, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/blog/datasets/">List of datasets</a></h1>
          <time class="published" datetime="">October 30, 2019</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/blog/japanese/suru/">Usages of する</a></h1>
          <time class="published" datetime="">October 23, 2019</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/blog/vectorizing-text/">Vectorizing text</a></h1>
          <time class="published" datetime="">October 17, 2019</time>
        </header>
      </article>
      
      
        <a href="/blog/" class="button">See more</a>
      
    </section>
  

  
    
      <section id="categories">
        <header>
          <h1><a href="/categories">Categories</a></h1>
        </header>
        <ul>
          
            
          
          
          <li>
            
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">3</span></a>
            
          
          <li>
            
              <a href="/categories/japanese/">japanese<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/machine-learning/">machine-learning<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/programming/">programming<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/linguistic/">linguistic<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/linux/">linux<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/random/">random<span class="count">1</span></a>
            
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>This website is a weblog were I write about computer science, machine learning, language learning.</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/adamoudad" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>

<li><a href="//stackoverflow.com/users/7013114/adam-oudad" target="_blank" rel="noopener" title="Stack Overflow" class="fab fa-stack-overflow"></a></li>








<li><a href="//medium.com/@adam.oudad" target="_blank" rel="noopener" title="Medium" class="fab fa-medium"></a></li>
<li><a href="//linkedin.com/in/adam-oudad-9436b866" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>















<li><a href="//twitter.com/OudadAdam" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>







<li><a href="//researchgate.net/profile/Adam_Oudad" target="_blank" rel="noopener" title="Research Gate"><i class="ai ai-researchgate"></i></a></li>




      </ul>
  
  <p class="copyright">
    
      &copy; 2020
      
        Adam Oudad
      
    . <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.67.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="/js/bundle.min.e0c399b948d3cf5c3a5e3badb94023d0b064a19ac39fd8cd79fb9f57fa4d1091.js" integrity="sha256-4MOZuUjTz1w6XjutuUAj0LBkoZrDn9jNefufV/pNEJE="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150494000-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>
