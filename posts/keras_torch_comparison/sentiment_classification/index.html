<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<title>Comparing Keras and PyTorch on sentiment classification - (Machine) Learning log.</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="monetization" content="$ilp.uphold.com/iN8Xkq2iHNHB">


<meta name="generator" content="Hugo 0.99.1" /><meta itemprop="name" content="Comparing Keras and PyTorch on sentiment classification">
<meta itemprop="description" content="After part one which covered an overview of Keras and PyTorch syntaxes, this is part two of how to switch between Keras and PyTorch. We will implement a neural network to classify movie reviews by sentiment.
 Keras is aimed at fast prototyping. It is designed to write less code, letting the developper focus on other tasks such as data preparation, processing, cleaning, etc PyTorch is aimed at modularity and versatility."><meta itemprop="datePublished" content="2021-03-20T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-03-20T00:00:00+00:00" />
<meta itemprop="wordCount" content="2194">
<meta itemprop="keywords" content="machine-learning,pytorch,keras," /><meta property="og:title" content="Comparing Keras and PyTorch on sentiment classification" />
<meta property="og:description" content="After part one which covered an overview of Keras and PyTorch syntaxes, this is part two of how to switch between Keras and PyTorch. We will implement a neural network to classify movie reviews by sentiment.
 Keras is aimed at fast prototyping. It is designed to write less code, letting the developper focus on other tasks such as data preparation, processing, cleaning, etc PyTorch is aimed at modularity and versatility." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adamoudad.github.io/posts/keras_torch_comparison/sentiment_classification/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-03-20T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-03-20T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Comparing Keras and PyTorch on sentiment classification"/>
<meta name="twitter:description" content="After part one which covered an overview of Keras and PyTorch syntaxes, this is part two of how to switch between Keras and PyTorch. We will implement a neural network to classify movie reviews by sentiment.
 Keras is aimed at fast prototyping. It is designed to write less code, letting the developper focus on other tasks such as data preparation, processing, cleaning, etc PyTorch is aimed at modularity and versatility."/>
<meta name="twitter:site" content="@OudadAdam"/>
<link rel="stylesheet" href="/css/bundle.min.262e62e9c1615dd1ac95d339cfc4ca2167aee72b71d853838920a22e7088b06b.css" integrity="sha256-Ji5i6cFhXdGsldM5z8TKIWeu5ytx2FODiSCiLnCIsGs=">
        <link rel="stylesheet" href="/css/add-on.css">
</head>

  <body>
    
<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/">
        
          
            posts
          
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu">
      
        
          
          
            <a href="/" class="link"><i class='fa fa-home'></i> Home</a>
          
        
      
        
          
          
            <a href="/about/" class="link"><i class='far fa-id-card'></i> About</a>
          
        
      
        
          
          
            <a href="/posts/" class="link"><i class='far fa-newspaper'></i> Posts</a>
          
        
      
        
          
          
            <a href="/categories/" class="link"><i class='fas fa-sitemap'></i> Categories</a>
          
        
      
      <a href="#share-menu" class="share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      

    </menu>
    

    <a href="#share-menu" class="share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="lang-menu" class="flyout-menu">
  <a href="#" lang="en" class="link active">English (en)</a>
  
    
      
    
      
        <a href="/fr" lang="fr" class="no-lang link">Fran√ßais (fr)</a>
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Comparing%20Keras%20and%20PyTorch%20on%20sentiment%20classification&amp;url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2fsentiment_classification%2f" target="_blank" rel="noopener" class="share-btn twitter">
        <i class="fab fa-twitter"></i><p>&nbsp;Twitter</p>
      </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2fsentiment_classification%2f&amp;title=Comparing%20Keras%20and%20PyTorch%20on%20sentiment%20classification" target="_blank" rel="noopener" class="share-btn linkedin">
            <i class="fab fa-linkedin"></i><p>&nbsp;LinkedIn</p>
          </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/"><img src="/avatar.png" class="circle" width="150" alt="Adam Oudad" /></a>
  <header>
    <h1>Adam Oudad</h1>
  </header>
  <main>
    <p>(Machine) Learning log.</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/adamoudad" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>

<li><a href="//stackoverflow.com/users/7013114/adam-oudad" target="_blank" rel="noopener" title="Stack Overflow" class="fab fa-stack-overflow"></a></li>








<li><a href="//medium.com/@adam.oudad" target="_blank" rel="noopener" title="Medium" class="fab fa-medium"></a></li>
<li><a href="//linkedin.com/in/adamoudad" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>















<li><a href="//twitter.com/OudadAdam" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>







<li><a href="//researchgate.net/profile/Adam_Oudad" target="_blank" rel="noopener" title="Research Gate"><i class="ai ai-researchgate"></i></a></li>





      </ul>
    </footer>
  
</section>

      <main id="site-main">
        <article class="post">
  <header>
  <div class="title">
    
        <h2><a href="/posts/keras_torch_comparison/sentiment_classification/">Comparing Keras and PyTorch on sentiment classification</a></h2>
    
    
</div>
  <div class="meta">
    <time class="published" datetime="2021-03-20 00:00:00 &#43;0000 UTC">
      March 20, 2021
    </time>
    <span class="author"></span>
    
      <p>11 minutes read</p>
    
  </div>
</header>

  <section id="socnet-share">
    




  
    
    <a href="//twitter.com/share?text=Comparing%20Keras%20and%20PyTorch%20on%20sentiment%20classification&amp;url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2fsentiment_classification%2f" target="_blank" rel="noopener" class="share-btn twitter">
        <i class="fab fa-twitter"></i><p>&nbsp;Twitter</p>
      </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2fsentiment_classification%2f&amp;title=Comparing%20Keras%20and%20PyTorch%20on%20sentiment%20classification" target="_blank" rel="noopener" class="share-btn linkedin">
            <i class="fab fa-linkedin"></i><p>&nbsp;LinkedIn</p>
          </a>
  


  </section>
  

  <div class="content">
    
<p>
After <a href="syntax">part one</a> which covered an overview of Keras and PyTorch syntaxes, this is part two of how to switch between Keras and PyTorch. We will implement a neural network to classify movie reviews by sentiment.</p>
<ul>
<li>Keras is aimed at fast prototyping. It is designed to write less code, letting the developper focus on other tasks such as data preparation, processing, cleaning, etc</li>
<li>PyTorch is aimed at modularity and versatility. Fine-grained control on the flow of computations</li>
</ul>
<p>
  Yet both frameworks are meant to do the same, that is training deep neural networks. The number of resources of &#34;rosetta stone&#34; style tutorials is very limited on internet, especially resources demonstrating practical examples, so I hope this will help demistifying the differences and similarities between the two frameworks. Now let&#39;s get started!</p>
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Dataset of IMDB movie reviews
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>We will use IMDB dataset, a popular toy dataset in machine learning, which consists of movie reviews from the IMDB website annotated by positive or negative sentiment.</p>
<p>
Both Keras and PyTorch have helper functions to download and load the IMDB dataset. So instead of using dataloaders as we saw in <a href="syntax">part one</a> we will use this code in which we import the IMDB dataset in Keras.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.datasets <span style="color:#f92672">import</span> imdb
</span></span><span style="display:flex;"><span>input_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
</span></span><span style="display:flex;"><span>(x_train, y_train), (x_val, y_val) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span>input_dim)
</span></span><span style="display:flex;"><span>print(len(x_train), <span style="color:#e6db74">&#34;Training sequences&#34;</span>)
</span></span><span style="display:flex;"><span>print(len(x_val), <span style="color:#e6db74">&#34;Validation sequences&#34;</span>)</span></span></code></pre></div>
</div>
<p>This creates a <code class="verbatim">data/</code> folder and downloads the dataset inside. It splits the data in half into training and validation sets. The output gives the number of sequences</p>
<pre class="example">
25000 Training sequences
25000 Validation sequences
</pre>
<p>
With <a href="https://pytorch.org/text/_modules/torchtext/datasets/imdb.html">torchtext, PyTorch can also download and load the IMDB dataset</a> in the same fashion. Yet, to make sure the data we feed to Keras and the data we feed to PyTorch is the same, I will simply adapt the downloaded data using Keras to PyTorch, so we are sure to have consistency between both implementations, especially in terms of data cleaning and how the text is tokenized.</p>
<p>
In addition, we will pad the movie reviews to have sequences of same length. This is important to train on batches as it is not possible to parallelize computation of the error gradient on sequences of variable length.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.sequence <span style="color:#f92672">import</span> pad_sequences
</span></span><span style="display:flex;"><span>maxlen <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
</span></span><span style="display:flex;"><span>x_train <span style="color:#f92672">=</span> pad_sequences(x_train, maxlen<span style="color:#f92672">=</span>maxlen)
</span></span><span style="display:flex;"><span>x_val <span style="color:#f92672">=</span> pad_sequences(x_val, maxlen<span style="color:#f92672">=</span>maxlen)</span></span></code></pre></div>
</div>
<p>We fix the length of reviews to 200 word tokens and the vocabulary to 20k different word tokens. The dataset is already cleaned and tokenized so we can directly feed it to the neural networks! Before proceeding, We may take a look at some examples in our dataset out of curiosity. We need to get the word vocabulary using <code>imdb.get_word_index</code> which will let us decode that bunch of integers from the dataset into plain english.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>index_offset <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>word_index <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>get_word_index(path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;imdb_word_index.json&#34;</span>)
</span></span><span style="display:flex;"><span>word_index <span style="color:#f92672">=</span> {k: (v <span style="color:#f92672">+</span> index_offset) <span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>word_index[<span style="color:#e6db74">&#34;&lt;START&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>word_index[<span style="color:#e6db74">&#34;&lt;UNK&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>word_index[<span style="color:#e6db74">&#34;&lt;UNUSED&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>index_to_word <span style="color:#f92672">=</span> { v: k <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_text</span>(sample, index_to_word):
</span></span><span style="display:flex;"><span> <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join([index_to_word[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> sample])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>recover_text(x_train[<span style="color:#ae81ff">50</span>], index_to_word)</span></span></code></pre></div>
</div>
<p>With the above code we can check that the sequence <code class="verbatim">1, 13, 165, 219, 14, 20, 33, 6, 750, ...</code> from the dataset corresponds to</p>
<div class="verse-block">
<p>&lt;START&gt; i actually saw this movie at a theater as soon as i handed the cashier my money she said two words i had never heard at a theater before or since no &lt;UNK&gt; as soon as i heard those words i should have just &lt;UNK&gt; bye bye to my cash and gone home but no foolishly i went in and watched the movie this movie didn&#39;t make anyone in the theater laugh not even once not even &lt;UNK&gt; mostly we sat there in stunned silence every ten minutes or so someone would yell this movie sucks the audience would applaud enthusiastically then sit there in stunned bored silence for another ten minutes &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; [‚Ä¶]</p>
</div>
<p>We can print the sentiment associated with this review, it is <code>y_train[50]</code> which is <code>0</code> for &#34;negative&#34;. Looks right.</p>
<p>
   But wait! We need to adapt our data for PyTorch. This code will do the job.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> TensorDataset, DataLoader
</span></span><span style="display:flex;"><span>train_data <span style="color:#f92672">=</span> TensorDataset(torch<span style="color:#f92672">.</span>from_numpy(x_train), torch<span style="color:#f92672">.</span>from_numpy(y_train))
</span></span><span style="display:flex;"><span>valid_data <span style="color:#f92672">=</span> TensorDataset(torch<span style="color:#f92672">.</span>from_numpy(x_val), torch<span style="color:#f92672">.</span>from_numpy(y_val))
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoader(train_data, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, batch_size<span style="color:#f92672">=</span>batch_size, drop_last<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>valid_loader <span style="color:#f92672">=</span> DataLoader(valid_data, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, batch_size<span style="color:#f92672">=</span>batch_size, drop_last<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)</span></span></code></pre></div>
</div>
<p>The <code>TensorDataset</code> class will convert our data to torch tensors, slice into batches, and  shuffle. Then with a <code>DataLoader</code> instance we will be able to iterate over the batches.</p>
<p>
A side note here, concerning the use of <code>DataLoader</code> instance to load the data. It seems to guarantee some numerical stability  during training, because my experimentations demonstrated that when manually shuffling and slicing the data from the <code>train_data</code> and <code>valid_data</code> tensors gave poor classification results, the model being unable to effectively train.</p>
<p>
   Good! The data is ready, let&#39;s define our classifiers.</p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
Classifier model
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<p>Our classifier is a bidirectional two-layers LSTM on top of an embedding layer, and followed by a dense layer which gives one output value. This output value gives the probability of a review being positive. the closer to zero, the more negative the review is predicted and the closer to one, the more positive the review is predicted. To obtain such probability, we use a sigmoid function, which takes the final output of the neural network and squeezes the value between 0 and 1.</p>
<p>
  We will define the two following  hyperparameters to make sure both implementations match.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span></span></span></code></pre></div>
</div>
<p>
  Let&#39;s start with Keras (credits to the official Keras documentation<sup class="footnote-reference"><a id="footnote-reference-1" href="#footnote-1">1</a></sup>.)</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Bidirectional, LSTM, Dense, Embedding
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>  Embedding(input_dim, <span style="color:#ae81ff">128</span>),
</span></span><span style="display:flex;"><span>  Bidirectional(LSTM(hidden_dim, return_sequences<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)),
</span></span><span style="display:flex;"><span>  Bidirectional(LSTM(hidden_dim)),
</span></span><span style="display:flex;"><span>  Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>summary()</span></span></code></pre></div>
</div>
<p>
  Now, let&#39;s implement the same in PyTorch. Hang in there, because there are some technical subtelties to be aware of. I want to show you first what would be a naive implementation</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTM</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __init__(self, input_dim, embedding_dim, hidden_dim):
</span></span><span style="display:flex;"><span>      super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>input_dim <span style="color:#f92672">=</span> input_dim
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>embedding_dim <span style="color:#f92672">=</span> embedding_dim
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(input_dim, embedding_dim)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim,
</span></span><span style="display:flex;"><span>                          num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, bidirectional<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, src):
</span></span><span style="display:flex;"><span>      batch_size <span style="color:#f92672">=</span> src<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>      output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(src)
</span></span><span style="display:flex;"><span>      output, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(output)
</span></span><span style="display:flex;"><span>       output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output)
</span></span><span style="display:flex;"><span>      output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(output)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span> output</span></span></code></pre></div>
</div>
<p>If you read <a href="syntax">part one</a>, there won&#39;t be any surprise in this code. In PyTorch we simply detail a bit more the computation whereas Keras infer them as we stack the layers.</p>
<p>
  Now the sad news is this implementation is not exactly the same as the one with Keras. And the main difference is initialization! It requires a fair amount of digging to make both frameworks agree on initializations. For example, you can see that the <code>Embedding</code> layer <a href="https://keras.io/api/layers/core_layers/embedding/">from Keras initialize its weights with a uniform distribution</a>, whereas it is <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">a normal distribution which is used by PyTorch</a>. Let&#39;s get straight to the point and use the following method to adapt PyTorch initialization to Keras. </p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_weights</span>(self):
</span></span><span style="display:flex;"><span>  self<span style="color:#f92672">.</span>encoder<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>uniform_(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>  ih <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;weight_ih&#39;</span> <span style="color:#f92672">in</span> name)
</span></span><span style="display:flex;"><span>  hh <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;weight_hh&#39;</span> <span style="color:#f92672">in</span> name)
</span></span><span style="display:flex;"><span>  b <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;bias&#39;</span> <span style="color:#f92672">in</span> name)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> ih:
</span></span><span style="display:flex;"><span>      nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform(t)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> hh:
</span></span><span style="display:flex;"><span>      nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>orthogonal(t)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> b:
</span></span><span style="display:flex;"><span>      nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant(t, <span style="color:#ae81ff">0</span>)</span></span></code></pre></div>
</div>
<p>It is a nice snippet taken from <code>torchMoji</code>, <a href="https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983">a PyTorch implementation of DeepMoji by HuggingFace</a>. We now have our updated BiLSTM class in PyTorch</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTM</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> __init__(self, input_dim, embedding_dim, hidden_dim):
</span></span><span style="display:flex;"><span>      super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>input_dim <span style="color:#f92672">=</span> input_dim
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>embedding_dim <span style="color:#f92672">=</span> embedding_dim
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(input_dim, embedding_dim)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim,
</span></span><span style="display:flex;"><span>                          num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, bidirectional<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>weight)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>init_weights()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_weights</span>(self):
</span></span><span style="display:flex;"><span>      ih <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;weight_ih&#39;</span> <span style="color:#f92672">in</span> name)
</span></span><span style="display:flex;"><span>      hh <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;weight_hh&#39;</span> <span style="color:#f92672">in</span> name)
</span></span><span style="display:flex;"><span>      b <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;bias&#39;</span> <span style="color:#f92672">in</span> name)
</span></span><span style="display:flex;"><span>      self<span style="color:#f92672">.</span>encoder<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>uniform_(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> ih:
</span></span><span style="display:flex;"><span>          nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform(t)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> hh:
</span></span><span style="display:flex;"><span>          nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>orthogonal(t)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> b:
</span></span><span style="display:flex;"><span>          nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant(t, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, src):
</span></span><span style="display:flex;"><span>      batch_size <span style="color:#f92672">=</span> src<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>      output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(src)
</span></span><span style="display:flex;"><span>      output, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(output)
</span></span><span style="display:flex;"><span>      output <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>tanh(output[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>      output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output)
</span></span><span style="display:flex;"><span>      output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(output)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span> output, <span style="color:#66d9ef">None</span></span></span></code></pre></div>
</div>
<p>
  Well‚Ä¶ As you can see, even for very basic architectures, transferring code is not an easy job. And even the details are quite important to achieve similar results, but bear with me, because this is the closest I could get to the Keras implementation.</p>
<p>
  A quick sanity check would be to verify the number of parameters of both implementations match. <code>model.summary()</code> in Keras outputs the following.</p>
<pre class="example">
 Model: &#34;sequential&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 128)         2560000   
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         98816     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128)               98816     
_________________________________________________________________
dense (Dense)                (None, 1)                 129       
=================================================================
Total params: 2,757,761
Trainable params: 2,757,761
Non-trainable params: 0
_________________________________________________________________
</pre>
<p>Printing the model in PyTorch with <code>print(model)</code> shows we have the same architecture.</p>
<pre class="example">
BiLSTM(
(encoder): Embedding(20000, 128)
(lstm): LSTM(128, 64, num_layers=2, bidirectional=True)
(linear): Linear(in_features=128, out_features=1, bias=True)
(activation): Sigmoid()
)
</pre>
<p>
      But this does not inform us on the number of parameters. This line gives what we want (<a href="https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/8">thanks to Federico Baldassare</a>)</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>requires_grad)</span></span></code></pre></div>
</div>
<p>This gives 2758785 parameters. Oh! The numbers of parameters are different!? After digging this issue, I found out PyTorch has a second bias tensor <a href="https://discuss.pytorch.org/t/why-lstm-has-two-bias-parameters/45025">for compatibility reasons with CuDNN in its implementation of the LSTM</a>. You can effectively check that the count difference is equal to <code class="verbatim">2,758,785 - 2,757,761 = 1024</code>, which is <code class="verbatim">2 * 4 * 2 * 64 = 1024</code>. where we multiply 64, the size of the bias vector of one LSTM cell by 2, because we have a forward and a backward LSTM, 2 again because we use two layers, and 4 which is the number of gates in the definition of the LSTM. Even with a basic architecture like this one, subtle yet important differences arise.</p>
<p>
    Now let&#39;s check how the implementations perform.</p>
</div>
</div>
<div id="outline-container-headline-3" class="outline-2">
<h2 id="headline-3">
Training
</h2>
<div id="outline-text-headline-3" class="outline-text-2">
<p>We will use the following parameters for training.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span></span></span></code></pre></div>
</div>
<p>
In Keras, this one line will do</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(x<span style="color:#f92672">=</span>x_train, y<span style="color:#f92672">=</span>y_train,
</span></span><span style="display:flex;"><span>        batch_size<span style="color:#f92672">=</span>batch_size, epochs<span style="color:#f92672">=</span>epochs,
</span></span><span style="display:flex;"><span>        validation_data<span style="color:#f92672">=</span>(x_val, y_val))</span></span></code></pre></div>
</div>
<p>
Now with PyTorch, remember that everything Keras does under the hood must be explicitly written. First is to initialize the logs, loss criterion, model, device and optimizer.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> BiLSTM(input_dim, embedding_dim, hidden_dim)
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BCELoss()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters())
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>batch_history <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;loss&#34;</span>: [],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;accuracy&#34;</span>: []
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>epoch_history <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;loss&#34;</span>: [],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;accuracy&#34;</span>: [],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;val_loss&#34;</span>: [],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;val_accuracy&#34;</span>: [],
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
</div>
<p>
What follows  is the training loop. I use the <code>tqdm</code> library <a href="https://towardsdatascience.com/training-models-with-a-progress-a-bar-2b664de3e13e">which I recommend for printing a nice progress bar in the same fashion as Keras</a>.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tqdm <span style="color:#f92672">import</span> tqdm, trange
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> trange(epochs, unit<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epoch&#34;</span>, desc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Train&#34;</span>):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tqdm(train_loader, desc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Train&#34;</span>) <span style="color:#66d9ef">as</span> tbatch:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, (samples, targets) <span style="color:#f92672">in</span> enumerate(tbatch):
</span></span><span style="display:flex;"><span>            model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>            samples <span style="color:#f92672">=</span> samples<span style="color:#f92672">.</span>to(device)<span style="color:#f92672">.</span>long()
</span></span><span style="display:flex;"><span>            targets <span style="color:#f92672">=</span> targets<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            model<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            predictions, _ <span style="color:#f92672">=</span> model(samples<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> criterion(predictions<span style="color:#f92672">.</span>squeeze(), targets<span style="color:#f92672">.</span>float())
</span></span><span style="display:flex;"><span>            acc <span style="color:#f92672">=</span> (predictions<span style="color:#f92672">.</span>round()<span style="color:#f92672">.</span>squeeze() <span style="color:#f92672">==</span> targets)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>            acc <span style="color:#f92672">=</span> acc <span style="color:#f92672">/</span> batch_size
</span></span><span style="display:flex;"><span>            loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>            batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]<span style="color:#f92672">.</span>append(acc)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            tbatch<span style="color:#f92672">.</span>set_postfix(loss<span style="color:#f92672">=</span>sum(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]),
</span></span><span style="display:flex;"><span>                               acc<span style="color:#f92672">=</span>sum(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]))</span></span></code></pre></div>
</div>
<p>
Notice the use of <code>model.train()</code> and <code>model.eval()</code> methods which set <code>model.training</code> to either <code>True</code> or <code>False</code>. It decides whether or not to keep the gradient during the forward pass, which will be used for optimizing the network in the backward pass.</p>
<p>
I used <code>torch.nn.utils.clip_grad_norm_(model.parameters(), 5)</code> to clip weight gradients to 5. I am not sure how Keras deals with gradient clipping but it is usually good practice to implement this technique to avoid the problem of exploding gradient.</p>
<p>
A final remark concerns the ordering of dimensions. As I explained in <a href="syntax">part one</a>, Keras expects by default batch dimension to be first while PyTorch expects it in second position. So with <code>samples.transpose(0, 1)</code> we effectively permute first and second dimensions to fit PyTorch data model.</p>
<p>
Now, likewise Keras, when we reach the end of an epoch, we want to evaluate the model.</p>
<div class="src src-python">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>epoch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]<span style="color:#f92672">.</span>append(sum(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]))
</span></span><span style="display:flex;"><span>epoch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]<span style="color:#f92672">.</span>append(sum(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Validation...&#34;</span>)
</span></span><span style="display:flex;"><span>val_loss, val_accuracy <span style="color:#f92672">=</span> validate(model, valid_loader)
</span></span><span style="display:flex;"><span>epoch_history[<span style="color:#e6db74">&#34;val_loss&#34;</span>]<span style="color:#f92672">.</span>append(val_loss)
</span></span><span style="display:flex;"><span>epoch_history[<span style="color:#e6db74">&#34;val_accuracy&#34;</span>]<span style="color:#f92672">.</span>append(val_accuracy)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>epoch_history<span style="color:#e6db74">=}</span><span style="color:#e6db74">&#34;</span>)</span></span></code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-4" class="outline-2">
<h2 id="headline-4">
Results
</h2>
<div id="outline-text-headline-4" class="outline-text-2">
<p>In the following tables I report results for each training epoch, averaged over the number of batches.</p>
<figure>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">loss (Keras)</th>
<th class="align-right">loss (PyTorch)</th>
<th class="align-right">accuracy (Keras)</th>
<th class="align-right">accuracy (PyTorch)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.4479</td>
<td class="align-right">0.4881</td>
<td class="align-right">0.7916</td>
<td class="align-right">0.7556</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.2855</td>
<td class="align-right">0.3962</td>
<td class="align-right">0.8880</td>
<td class="align-right">0.8173</td>
</tr>
</tbody>
</table>
<figcaption>
Epoch results on the training set.
</figcaption>
</figure>
<figure>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">val_loss (Keras)</th>
<th class="align-right">val_loss (PyTorch)</th>
<th class="align-right">val_accuracy (Keras)</th>
<th class="align-right">val_accuracy (PyTorch)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.3639</td>
<td class="align-right">0.4024</td>
<td class="align-right">0.8440</td>
<td class="align-right">0.8233</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.3468</td>
<td class="align-right">0.3343</td>
<td class="align-right">0.8542</td>
<td class="align-right">0.8624</td>
</tr>
</tbody>
</table>
<figcaption>
Epoch results on the validation set.
</figcaption>
</figure>
<p>
  The following graphs show the loss and binary accuracy on batch during training.
<figure><img src="../train_loss.png"
         alt="Batch training loss"/><figcaption>
            <p>Batch training loss</p>
        </figcaption>
</figure>

<figure><img src="../train_accuracy.png"
         alt="Batch training accuracy"/><figcaption>
            <p>Batch training accuracy</p>
        </figcaption>
</figure>
</p>
<p>
We can notice a sudden jump in accuracy and loss for the Keras implementation. This may be reduced if we optimize the learning rate.</p>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
Last words
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<p>Even with great care, it is hard to obtain the very same results with identical architecture, data processing and training, because each framework has its own internal mechanisms, designs, with in addition specific tricks to guarantee numerical stability. Nonetheless, the results we got are close.</p>
<p>
  There are ambitious projects such as <a href="https://onnx.ai/">ONNX</a> creating a standard between deep learning frameworks, to practically solve the discrepancies we witnessed in this article.</p>
<p>
  I hope you enjoyed this comparison of Keras and PyTorch want to read your ideas and feedbacks! Hope you the best in your machine learning journey!</p>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
Footnotes
</h2>
</div>
<div class="footnotes">
<hr class="footnotes-separatator">
<div class="footnote-definitions">
<div class="footnote-definition">
<sup id="footnote-1"><a href="#footnote-reference-1">1</a></sup>
<div class="footnote-body">
<p><a href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/">https://keras.io/examples/nlp/bidirectional_lstm_imdb/</a></p>
</div>
</div>
</div>
</div>

  </div>
  <footer>
    <ul class="stats">
  
    
    
      <li class="categories">
        <ul>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/categories/machine-learning">Machine Learning</a></li>
          
        </ul>
      </li>
    
  
  
    
    
      <li class="tags">
        <ul>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/tags/machine-learning">machine-learning</a></li>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/tags/pytorch">pytorch</a></li>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/tags/keras">keras</a></li>
          
        </ul>
      </li>
    
  
</ul>

  </footer>
</article>

    <article class="post">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "adamoudad-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </article>




<div class="pagination">
  
    <a href="/posts/keras_torch_comparison/syntax/" class="button"><div class="previous"><div>Comparison of Keras and PyTorch syntaxes</div></div></a>
  
  
    <a href="/posts/swap_control_caps/" class="button"><div class="next"><div>Swap Control and Caps keys to relieve stress on the pinky</div></div></a>
  
</div>


      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent posts</h1>
      </header>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/emacs/remote-command-ssh/">Run a command on a remote server with Emacs.</a></h1>
          <time class="published" datetime="">December 22, 2022</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/python-statements/">These Python keywords will simplify your code</a></h1>
          <time class="published" datetime="">October 16, 2022</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/pdfpc/">Keynote presentation on Linux</a></h1>
          <time class="published" datetime="">October 15, 2022</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/swap_control_caps/">Swap Control and Caps keys to relieve stress on the pinky</a></h1>
          <time class="published" datetime="">April 5, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/keras_torch_comparison/sentiment_classification/">Comparing Keras and PyTorch on sentiment classification</a></h1>
          <time class="published" datetime="">March 20, 2021</time>
        </header>
      </article>
      
      
        <a href="/posts/" class="button">See more</a>
      
    </section>
  

  
    
      <section id="categories">
        <header>
          <h1><a href="/categories">Categories</a></h1>
        </header>
        <ul>
          
            
          
          
          <li>
            
              <a href="/categories/machine-learning/">machine-learning<span class="count">9</span></a>
            
          
          <li>
            
              <a href="/categories/programming/">programming<span class="count">6</span></a>
            
          
          <li>
            
              <a href="/categories/linux/">linux<span class="count">3</span></a>
            
          
          <li>
            
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">3</span></a>
            
          
          <li>
            
              <a href="/categories/python/">python<span class="count">3</span></a>
            
          
          <li>
            
              <a href="/categories/emacs/">emacs<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/japanese/">japanese<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/security/">security<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/web/">web<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/computer/">computer<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/linguistic/">linguistic<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/random/">random<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/tools/">tools<span class="count">1</span></a>
            
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>This website is a weblog were I write about computer science, machine learning, language learning.</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/adamoudad" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>

<li><a href="//stackoverflow.com/users/7013114/adam-oudad" target="_blank" rel="noopener" title="Stack Overflow" class="fab fa-stack-overflow"></a></li>








<li><a href="//medium.com/@adam.oudad" target="_blank" rel="noopener" title="Medium" class="fab fa-medium"></a></li>
<li><a href="//linkedin.com/in/adamoudad" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>















<li><a href="//twitter.com/OudadAdam" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>







<li><a href="//researchgate.net/profile/Adam_Oudad" target="_blank" rel="noopener" title="Research Gate"><i class="ai ai-researchgate"></i></a></li>





      </ul>
  
  <p class="copyright">
    
      &copy; 2022
      
        (Machine) Learning log.
      
    . <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.99.1' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


      <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="/js/bundle.min.d0b3e0b5f2cfc7467ead7d316ecb9dea4d29ef3c23ad300df9d7017ff98b2331.js" integrity="sha256-0LPgtfLPx0Z&#43;rX0xbsud6k0p7zwjrTAN&#43;dcBf/mLIzE="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150494000-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>
