<!doctype html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<title>Guide to swtich between Keras and PyTorch - Adam Oudad</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="monetization" content="$ilp.uphold.com/iN8Xkq2iHNHB">


<meta name="generator" content="Hugo 0.80.0" /><meta itemprop="name" content="Guide to swtich between Keras and PyTorch">
<meta itemprop="description" content="Introduction of deep learning frameworks  Introduction of deep learning frameworks    Keras is developped by Google. It is included inside Tensorflow. Last version is 2.3.0
  Pytorch is developped by Facebook. Last version is 1.6.0
  Pytorch lightning is for fast prototyping (similar to Keras) 1
  Pytorch Ignite is another fast protoyping library for PyTorch. It is developped by PyTorch team.2
      In this tutorial, we&#39;ll see how to swtich between Keras and Pytorch code">
<meta itemprop="datePublished" content="2021-01-30T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-01-30T00:00:00+00:00" />
<meta itemprop="wordCount" content="3832">



<meta itemprop="keywords" content="machine-learning,pytorch,keras," />
<meta property="og:title" content="Guide to swtich between Keras and PyTorch" />
<meta property="og:description" content="Introduction of deep learning frameworks  Introduction of deep learning frameworks    Keras is developped by Google. It is included inside Tensorflow. Last version is 2.3.0
  Pytorch is developped by Facebook. Last version is 1.6.0
  Pytorch lightning is for fast prototyping (similar to Keras) 1
  Pytorch Ignite is another fast protoyping library for PyTorch. It is developped by PyTorch team.2
      In this tutorial, we&#39;ll see how to swtich between Keras and Pytorch code" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://adamoudad.github.io/posts/keras_torch_comparison/" />
<meta property="article:published_time" content="2021-01-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-01-30T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Guide to swtich between Keras and PyTorch"/>
<meta name="twitter:description" content="Introduction of deep learning frameworks  Introduction of deep learning frameworks    Keras is developped by Google. It is included inside Tensorflow. Last version is 2.3.0
  Pytorch is developped by Facebook. Last version is 1.6.0
  Pytorch lightning is for fast prototyping (similar to Keras) 1
  Pytorch Ignite is another fast protoyping library for PyTorch. It is developped by PyTorch team.2
      In this tutorial, we&#39;ll see how to swtich between Keras and Pytorch code"/>
<meta name="twitter:site" content="@OudadAdam"/>
<link rel="stylesheet" href="/css/bundle.min.a86839252b4aa4853111ceba45af5da1d6c3dc66a6a3b6854750bb2df61dfe37.css" integrity="sha256-qGg5JStKpIUxEc66Ra9dodbD3Gamo7aFR1C7LfYd/jc=">
        <link rel="stylesheet" href="/css/add-on.css">
</head>

  <body>
    
<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/">
        
          
            posts
          
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu">
      
        
          
          
            <a href="/" class="link"><i class='fa fa-home'></i> Home</a>
          
        
      
        
          
          
            <a href="/about/" class="link"><i class='far fa-id-card'></i> About</a>
          
        
      
        
          
          
            <a href="/posts/" class="link"><i class='far fa-newspaper'></i> Posts</a>
          
        
      
        
          
          
            <a href="/categories/" class="link"><i class='fas fa-sitemap'></i> Categories</a>
          
        
      
      <a href="#share-menu" class="share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      

    </menu>
    

    <a href="#share-menu" class="share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="lang-menu" class="flyout-menu">
  <a href="#" lang="en" class="link active">English (en)</a>
  
    
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Guide%20to%20swtich%20between%20Keras%20and%20PyTorch&amp;url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2f" target="_blank" rel="noopener" class="share-btn twitter">
        <i class="fab fa-twitter"></i><p>&nbsp;Twitter</p>
      </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2f&amp;title=Guide%20to%20swtich%20between%20Keras%20and%20PyTorch" target="_blank" rel="noopener" class="share-btn linkedin">
            <i class="fab fa-linkedin"></i><p>&nbsp;LinkedIn</p>
          </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  <a href="/"><img src="/avatar.png" class="circle" width="150" alt="Adam Oudad" /></a>
  <header>
    <h1>Adam Oudad</h1>
  </header>
  <main>
    <p>(Machine) Learning log.</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/adamoudad" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>

<li><a href="//stackoverflow.com/users/7013114/adam-oudad" target="_blank" rel="noopener" title="Stack Overflow" class="fab fa-stack-overflow"></a></li>








<li><a href="//medium.com/@adam.oudad" target="_blank" rel="noopener" title="Medium" class="fab fa-medium"></a></li>
<li><a href="//linkedin.com/in/adam-oudad-9436b866" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>















<li><a href="//twitter.com/OudadAdam" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>







<li><a href="//researchgate.net/profile/Adam_Oudad" target="_blank" rel="noopener" title="Research Gate"><i class="ai ai-researchgate"></i></a></li>





      </ul>
    </footer>
  
</section>

      <main id="site-main">
        <article class="post">
  <header>
  <div class="title">
    
        <h2><a href="/posts/keras_torch_comparison/">Guide to swtich between Keras and PyTorch</a></h2>
    
    
</div>
  <div class="meta">
    <time class="published" datetime="2021-01-30 00:00:00 &#43;0000 UTC">
      January 30, 2021
    </time>
    <span class="author"></span>
    
      <p>18 minutes read</p>
    
  </div>
</header>

  <section id="socnet-share">
    




  
    
    <a href="//twitter.com/share?text=Guide%20to%20swtich%20between%20Keras%20and%20PyTorch&amp;url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2f" target="_blank" rel="noopener" class="share-btn twitter">
        <i class="fab fa-twitter"></i><p>&nbsp;Twitter</p>
      </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fadamoudad.github.io%2fposts%2fkeras_torch_comparison%2f&amp;title=Guide%20to%20swtich%20between%20Keras%20and%20PyTorch" target="_blank" rel="noopener" class="share-btn linkedin">
            <i class="fab fa-linkedin"></i><p>&nbsp;LinkedIn</p>
          </a>
  


  </section>
  

  <div class="content">
    
<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
Introduction of deep learning frameworks
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<div id="outline-container-headline-2" class="outline-3">
<h3 id="headline-2">
Introduction of deep learning frameworks
</h3>
<div id="outline-text-headline-2" class="outline-text-3">
<ul>
<li>
<p>Keras is developped by Google. It is included inside Tensorflow. Last version is 2.3.0</p>
</li>
<li>
<p>Pytorch is developped by Facebook. Last version is 1.6.0</p>
<ul>
<li>
<p>Pytorch lightning is for fast prototyping (similar to Keras) <sup class="footnote-reference"><a id="footnote-reference-1" href="#footnote-1">1</a></sup></p>
</li>
<li>
<p>Pytorch Ignite is another fast protoyping library for PyTorch. It is developped by PyTorch team.<sup class="footnote-reference"><a id="footnote-reference-2" href="#footnote-2">2</a></sup></p>
</li>
</ul>
</li>
</ul>
<p><img src="~/Pictures/images/switch_keras_pytorch/deeplearning_frameworks.png" alt="~/Pictures/images/switch_keras_pytorch/deeplearning_frameworks.png" title="~/Pictures/images/switch_keras_pytorch/deeplearning_frameworks.png" /></p>
<ul>
<li>
<p>In this tutorial, we&#39;ll see how to swtich between Keras and Pytorch code</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-3" class="outline-2">
<h2 id="headline-3">
Datasets
</h2>
<div id="outline-text-headline-3" class="outline-text-2">
<p>Keras and PyTorch both come with a library useful for loading some toy datasets, such as MNIST, IMDB, â€¦
  We will use the IMDB dataset in part 2 to compare frameworks.
  Data loading also can be achieved similarly in a similar way. Keras has <code>utils.Sequence</code> class, while PyTorch has <code>utils.dataset</code>.</p>
<p>
  This tutorial shows data loading with Keras and PyTorch <a href="https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/">https://deepsense.ai/keras-vs-pytorch-avp-transfer-learning/</a></p>
</div>
</div>
<div id="outline-container-headline-4" class="outline-2">
<h2 id="headline-4">
COMMENT Data processing and loading (PyTorch)
</h2>
<div id="outline-text-headline-4" class="outline-text-2">
<ul>
<li>
<p>PyTorch has datasets library</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> Dataset
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CustomDataset</span>(Dataset):
<span style="color:#66d9ef">def</span> __init__(self, path):
    self<span style="color:#f92672">.</span>path <span style="color:#f92672">=</span> Path(path)
    self<span style="color:#f92672">.</span>filenames <span style="color:#f92672">=</span> list(self<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>glob(<span style="color:#e6db74">&#34;**/*.mid&#34;</span>))

<span style="color:#66d9ef">def</span> __len__(self):
    <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>filenames)

<span style="color:#66d9ef">def</span> __getitem__(self, index):
    fn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>filenames[index]
    vector <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>load(fn)
    <span style="color:#66d9ef">return</span> vector</code></pre></div>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-headline-5" class="outline-2">
<h2 id="headline-5">
COMMENT Data processing and loading (Keras)
</h2>
<div id="outline-text-headline-5" class="outline-text-2">
<ul>
<li>
<p>Keras uses Sequence class</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.utils <span style="color:#f92672">import</span> Sequence
<span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PianorollGenerator</span>(Sequence):
<span style="color:#66d9ef">def</span> __init__(self, path):
    self<span style="color:#f92672">.</span>path <span style="color:#f92672">=</span> Path(path)
    self<span style="color:#f92672">.</span>filenames <span style="color:#f92672">=</span> list(self<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>glob(<span style="color:#e6db74">&#34;**/*.mid&#34;</span>))

<span style="color:#66d9ef">def</span> __len__(self):
    <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>filenames)

<span style="color:#66d9ef">def</span> __getitem__(self, index):
    fn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>filenames[index]
    vector <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>load(fn)
    <span style="color:#66d9ef">return</span> vector</code></pre></div>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-headline-6" class="outline-2">
<h2 id="headline-6">
COMMENT Build models (Keras)
</h2>
<div id="outline-text-headline-6" class="outline-text-2">
<div id="outline-container-headline-7" class="outline-3">
<h3 id="headline-7">
Build models (Keras)
</h3>
<div id="outline-text-headline-7" class="outline-text-3">
<ul>
<li>
<p>One popular way to build your Keras model is to use a build function, that is function which instantiates and builds a model and returns it<sup class="footnote-reference"><a id="footnote-reference-3" href="#footnote-3">3</a></sup></p>
</li>
</ul>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
<span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> LSTM, Dense, Embedding
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_model</span>(vocab_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
             hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>):
 model <span style="color:#f92672">=</span> Sequential()
 model<span style="color:#f92672">.</span>add(Embedding(vocab_size, embedding_dim))
 model<span style="color:#f92672">.</span>add(LSTM(hidden_size))
 model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>))
 model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;adam&#34;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary_crossentropy&#34;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
 <span style="color:#66d9ef">return</span> model</code></pre></div>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-8" class="outline-2">
<h2 id="headline-8">
COMMENT Build models (Pytorch)
</h2>
<div id="outline-text-headline-8" class="outline-text-2">
<div id="outline-container-headline-9" class="outline-3">
<h3 id="headline-9">
Build models (Pytorch)
</h3>
<div id="outline-text-headline-9" class="outline-text-3">
<ul>
<li>
<p>This is the same model in Pytorch<sup class="footnote-reference"><a id="footnote-reference-4" href="#footnote-4">4</a></sup> <sup class="footnote-reference"><a id="footnote-reference-5" href="#footnote-5">5</a></sup></p>
</li>
</ul>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn
<span style="color:#f92672">from</span> torch.nn.functional <span style="color:#f92672">import</span> softmax
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CustomModel</span>(nn<span style="color:#f92672">.</span>Module):
 <span style="color:#66d9ef">def</span> __init__(self, vocab_size<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,
              embedding_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
              hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>):
     super()<span style="color:#f92672">.</span>__init__()
     self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, embedding_dim)
     self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_size)
     self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_size, <span style="color:#ae81ff">1</span>)

 <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(x)
     output, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(output)
     output <span style="color:#f92672">=</span> output[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#75715e"># Keep last output only</span>
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output)
     output <span style="color:#f92672">=</span> softmax(output)
     <span style="color:#66d9ef">return</span> output</code></pre></div>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-10" class="outline-2">
<h2 id="headline-10">
COMMENT Build Transformer for sentiment classification
</h2>
<div id="outline-text-headline-10" class="outline-text-2">
<p>Keras: <a href="https://keras.io/examples/nlp/text_classification_with_transformer/">https://keras.io/examples/nlp/text_classification_with_transformer/</a>
  PyTorch: <a href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a></p>
</div>
</div>
<div id="outline-container-headline-11" class="outline-2">
<h2 id="headline-11">
Keras and PyTorch compared on sentiment classification
</h2>
<div id="outline-text-headline-11" class="outline-text-2">
<p>Keras and PyTorch are two popular frameworks for training deep neural networks. They are both reputed for their speed and stability. Their respective approach, however is different</p>
<ul>
<li>
<p>Keras is aimed at fast prototyping. It is designed to write less code, letting the developper focus on other tasks such as data preparation, processing, cleaning, etc</p>
</li>
<li>
<p>PyTorch is aimed at modularity and versatility. You can virtually implement any neural network architecture with it, and have fine-grained control on its computation flow. For example, it has introduced the dynamic computational graph, which lets you modify the computation flow as the program is running.</p>
</li>
</ul>
<p>
  As you see, Keras and PyTorch are, strictly speaking, not really comparable. Keras is based on Tensorflow, which is closer to PyTorch in its approach. However, we can see a trend in term of popularity, in which Keras is favored by prototypers crafting proof of concept and people new to deep learning, while PyTorch tends to be favored by deep learning researchers. I noticed a lot of people wanting to go from one framework to the other, but the number of resources demonstrating practical examples in both frameworks is very limited.</p>
<p>
  So the goal of this tutorial is to present you a comparison of Keras and PyTorch on a toy dataset, the IMDB review dataset, annotated with sentiment. We will use a neural network architecture based on LSTM networks to process an input sentence pre-processed to word tokens, to perform sentiment classification. The tutorial is in three parts</p>
<ol>
<li>
<p>Data preparation</p>
</li>
<li>
<p>Define the model</p>
</li>
<li>
<p>Training</p>
</li>
<li>
<p>Results</p>
</li>
</ol>
<div id="outline-container-headline-12" class="outline-3">
<h3 id="headline-12">
Data preparation
</h3>
<div id="outline-text-headline-12" class="outline-text-3">
<p>The IMDB dataset can be imported in <a href="https://keras.io/api/datasets/imdb/">Keras with</a></p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">   <span style="color:#f92672">from</span> tensorflow.keras.datasets <span style="color:#f92672">import</span> imdb
   input_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
   (x_train, y_train), (x_val, y_val) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span>input_dim)</code></pre></div>
</div>
<p>or <a href="https://pytorch.org/text/_modules/torchtext/datasets/imdb.html">PyTorch via torchtext with</a></p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torchtext.datasets <span style="color:#f92672">import</span> IMDB
<span style="color:#f92672">from</span> torchtext <span style="color:#f92672">import</span> data
TEXT <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>Field(lower<span style="color:#f92672">=</span>True, include_lengths<span style="color:#f92672">=</span>True, batch_first<span style="color:#f92672">=</span>True)
LABEL <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>Field(sequential<span style="color:#f92672">=</span>False)
train, test <span style="color:#f92672">=</span> IMDB<span style="color:#f92672">.</span>splits(TEXT, LABEL)</code></pre></div>
</div>
<p>They both essentially contain the same data. But for the purpose of comparing frameworks, we will use the Keras version of the dataset, to have a consistent pre-processing between experiments. This is the data preparation code.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.datasets <span style="color:#f92672">import</span> imdb
<span style="color:#f92672">from</span> tensorflow.keras.preprocessing.sequence <span style="color:#f92672">import</span> pad_sequences

input_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
maxlen <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
(x_train, y_train), (x_val, y_val) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span>input_dim)
<span style="color:#66d9ef">print</span>(len(x_train), <span style="color:#e6db74">&#34;Training sequences&#34;</span>)
<span style="color:#66d9ef">print</span>(len(x_val), <span style="color:#e6db74">&#34;Validation sequences&#34;</span>)
x_train <span style="color:#f92672">=</span> pad_sequences(x_train, maxlen<span style="color:#f92672">=</span>maxlen)
x_val <span style="color:#f92672">=</span> pad_sequences(x_val, maxlen<span style="color:#f92672">=</span>maxlen)</code></pre></div>
</div>
<p>We fix the maximum length of reviews to 200 words and the vocabulary to 20k words. The good news is that the dataset is already cleaned and tokenized, so we can directly feed it to the neural networks we will define next. It is good to check that the data we feed makes sense. Let&#39;s recover a data sample using the </p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">index_offset <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
word_index <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>get_word_index(path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;imdb_word_index.json&#34;</span>)
word_index <span style="color:#f92672">=</span> {k: (v <span style="color:#f92672">+</span> index_offset) <span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()}
word_index[<span style="color:#e6db74">&#34;&lt;PAD&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
word_index[<span style="color:#e6db74">&#34;&lt;START&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
word_index[<span style="color:#e6db74">&#34;&lt;UNK&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
word_index[<span style="color:#e6db74">&#34;&lt;UNUSED&gt;&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
index_to_word <span style="color:#f92672">=</span> { v: k <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> word_index<span style="color:#f92672">.</span>items()}

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_text</span>(sample, index_to_word):
 <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39; &#39;</span><span style="color:#f92672">.</span>join([index_to_word[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> sample])

recover_text(x_train[<span style="color:#ae81ff">50</span>], index_to_word)</code></pre></div>
</div>
<pre class="example">
&lt;START&gt; i actually saw this movie at a theater as soon as i handed the cashier my money she said two words i had never heard at a theater before or since no &lt;UNK&gt; as soon as i heard those words i should have just &lt;UNK&gt; bye bye to my cash and gone home but no foolishly i went in and watched the movie this movie didn&#39;t make anyone in the theater laugh not even once not even &lt;UNK&gt; mostly we sat there in stunned silence every ten minutes or so someone would yell this movie sucks the audience would applaud enthusiastically then sit there in stunned bored silence for another ten minutes &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt;
</pre>
<p>We can print the sentiment associated with this review, it is <code>y_train[50]</code> which is <code>0</code> for &#34;negative&#34;. Seems about right.</p>
<p>
   Good! The data is ready, let&#39;s head over the models.</p>
</div>
</div>
<div id="outline-container-headline-13" class="outline-3">
<h3 id="headline-13">
Define the model
</h3>
<div id="outline-text-headline-13" class="outline-text-3">
<p>The model consists in a bidirectional two-layers LSTM on top of an embedding layer, and followed by a dense layer with one output value, and with a sigmoid activation. The implementation relies on the following hyperparameters</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span></code></pre></div>
</div>
<p>We can start with Keras, which has a very short and concise code for this.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Bidirectional, LSTM, Dense, Embedding
<span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
model <span style="color:#f92672">=</span> Sequential([
 Embedding(input_dim, <span style="color:#ae81ff">128</span>, mask_zero<span style="color:#f92672">=</span>False),
 Bidirectional(LSTM(hidden_dim, return_sequences<span style="color:#f92672">=</span>True)),
 Bidirectional(LSTM(hidden_dim)),
 Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)
])
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
model<span style="color:#f92672">.</span>summary()</code></pre></div>
</div>
<p>I credit the implementation to this tutorial from the official documentation of Keras<sup class="footnote-reference"><a id="footnote-reference-6" href="#footnote-6">6</a></sup>.
   Now, let&#39;s implement the same in PyTorch.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> torch <span style="color:#f92672">import</span> nn
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTM</span>(nn<span style="color:#f92672">.</span>Module):
 <span style="color:#66d9ef">def</span> __init__(self, input_dim, embedding_dim, hidden_dim):
     super()<span style="color:#f92672">.</span>__init__()
     self<span style="color:#f92672">.</span>input_dim <span style="color:#f92672">=</span> input_dim
     self<span style="color:#f92672">.</span>embedding_dim <span style="color:#f92672">=</span> embedding_dim
     self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
     self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(input_dim, embedding_dim)
     self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim,
                         num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, bidirectional<span style="color:#f92672">=</span>True)
     self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
     self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()

     initrange <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
     self<span style="color:#f92672">.</span>encoder<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>uniform_(<span style="color:#f92672">-</span>initrange, initrange)
     nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>weight)  <span style="color:#75715e"># Same as glorot uniform in pytorch</span>
     self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
     self<span style="color:#f92672">.</span>init_weights()

 <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_weights</span>(self):
     <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">     Here we reproduce Keras default initialization weights to initialize Embeddings/LSTM weights
</span><span style="color:#e6db74">     Source: https://gist.github.com/thomwolf/eea8989cab5ac49919df95f6f1309d80
</span><span style="color:#e6db74">     &#34;&#34;&#34;</span>
     ih <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;weight_ih&#39;</span> <span style="color:#f92672">in</span> name)
     hh <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;weight_hh&#39;</span> <span style="color:#f92672">in</span> name)
     b <span style="color:#f92672">=</span> (param<span style="color:#f92672">.</span>data <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>named_parameters() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;bias&#39;</span> <span style="color:#f92672">in</span> name)
     <span style="color:#75715e"># nn.init.uniform(self.embed.weight.data, a=-0.5, b=0.5) # Already doing this in init</span>
     <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> ih:
         nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform(t)
     <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> hh:
         nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>orthogonal(t)
     <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> b:
         nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant(t, <span style="color:#ae81ff">0</span>)

 <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, src):
     batch_size <span style="color:#f92672">=</span> src<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(src)
     output, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(output)
     output <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>tanh(output[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output)
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(output)
     <span style="color:#66d9ef">return</span> output, None</code></pre></div>
</div>
<p>Wellâ€¦ As you can see transferring code is not an easy job. I had to extensively research on how Keras initialize weights and biases, and what activation functions it chooses by default. I used a code implementation <a href="https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983">from HuggingFace in which they ported a tensorflow implementation of an LSTM to PyTorch</a>. In particular the <code>init_weights</code> method is very handy for mimicking Keras&#39;s way of initializing the network&#39;s parameters. The details are quite important to achieve similar results, but bear with me, this is the closest I could get to the Keras implementation.</p>
<p>
   A quick sanity check for our two implementations is the number of parameters</p>
<ul>
<li>
<p><code>model.summary()</code> in Keras gives</p>
<pre class="example">
Model: &#34;sequential&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 128)         2560000   
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         98816     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128)               98816     
_________________________________________________________________
dense (Dense)                (None, 1)                 129       
=================================================================
Total params: 2,757,761
Trainable params: 2,757,761
Non-trainable params: 0
_________________________________________________________________
</pre>
</li>
<li>
<p><code>model</code> and model.count_parameters() in PyTorch gives</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">count_parameters</span>(model):
<span style="color:#66d9ef">return</span> sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters() <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>requires_grad)</code></pre></div>
</div>
<p>Thanks to Federico Baldassare for this <a href="https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/8">https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/8</a></p>
<pre class="example">
BiLSTM(
(encoder): Embedding(20000, 128)
(lstm): LSTM(128, 64, num_layers=2, bidirectional=True)
(linear): Linear(in_features=128, out_features=1, bias=True)
(activation): Sigmoid()
)
</pre>
<p>This outputs 2758785</p>
<p>
  What is this!! The numbers of parameters are different!? This seems to come from compatibility reasons with CuDNN on the LSTM implementation. You can check that the count difference is equal to <code class="verbatim">2 * 4 * 2 * 64 = 1024</code>. 2 for bidirectional, 2 for the number of layers, 4 for the 4 different gates of the LSTM and 64 for the actual hidden size. We will stop here for the explanation but you can see that very early some notable implementation differences arise so targeting the exact same model architecture in both framework is very hard.</p>
</li>
</ul>
<p>We have implemented our models, let&#39;s train them!</p>
</div>
</div>
<div id="outline-container-headline-14" class="outline-3">
<h3 id="headline-14">
Training
</h3>
<div id="outline-text-headline-14" class="outline-text-3">
<p>We will use the following parameters for training.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>
epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span></code></pre></div>
</div>
<p>In Keras, well, we have a one-liner!</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model<span style="color:#f92672">.</span>fit(x<span style="color:#f92672">=</span>x_train, y<span style="color:#f92672">=</span>y_train,
          batch_size<span style="color:#f92672">=</span>batch_size, epochs<span style="color:#f92672">=</span>epochs,
          validation_data<span style="color:#f92672">=</span>(x_val, y_val),
          shuffle<span style="color:#f92672">=</span>True)</code></pre></div>
</div>
<p>And as you can expect, brace yourself for PyTorch, because we need to do several things. First, we may adapt the dataset. PyTorch has a <code>TensorDataset</code> class which will convert our data to torch tensors and slice them into batches and shuffle them. Then we need <code>DataLoader</code> instance which will provide a ready to use batch.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> TensorDataset, DataLoader
train_data <span style="color:#f92672">=</span> TensorDataset(torch<span style="color:#f92672">.</span>from_numpy(x_train), torch<span style="color:#f92672">.</span>from_numpy(y_train))
valid_data <span style="color:#f92672">=</span> TensorDataset(torch<span style="color:#f92672">.</span>from_numpy(x_val), torch<span style="color:#f92672">.</span>from_numpy(y_val))
train_loader <span style="color:#f92672">=</span> DataLoader(train_data, shuffle<span style="color:#f92672">=</span>True, batch_size<span style="color:#f92672">=</span>batch_size, drop_last<span style="color:#f92672">=</span>True)
valid_loader <span style="color:#f92672">=</span> DataLoader(valid_data, shuffle<span style="color:#f92672">=</span>True, batch_size<span style="color:#f92672">=</span>batch_size, drop_last<span style="color:#f92672">=</span>True)</code></pre></div>
</div>
<p>Let&#39;s initialize the logs, loss criterion, model, device and optimizer.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> BiLSTM(input_dim, embedding_dim, hidden_dim)
device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BCELoss()<span style="color:#f92672">.</span>to(device)
optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters())
model<span style="color:#f92672">.</span>to(device)
batch_history <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;loss&#34;</span>: [],
    <span style="color:#e6db74">&#34;accuracy&#34;</span>: []
}
epoch_history <span style="color:#f92672">=</span> {
    <span style="color:#e6db74">&#34;loss&#34;</span>: [],
    <span style="color:#e6db74">&#34;accuracy&#34;</span>: [],
    <span style="color:#e6db74">&#34;val_loss&#34;</span>: [],
    <span style="color:#e6db74">&#34;val_accuracy&#34;</span>: [],
}</code></pre></div>
</div>
<p>Everything Keras does under the hood must be explicitly written in PyTorch. This means</p>
<ul>
<li>
<p>loading a batchmodel </p>
</li>
</ul>
<p>I use the <code>tqdm</code> library which I recommend for printing a nice progress bar in the same fashion as Keras. I have written <a href="https://towardsdatascience.com/training-models-with-a-progress-a-bar-2b664de3e13e">a quick article on how to use it</a> for your information!</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tqdm <span style="color:#f92672">import</span> tqdm, trange
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> trange(epochs, unit<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epoch&#34;</span>, desc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Train&#34;</span>):
    model<span style="color:#f92672">.</span>train()
    <span style="color:#66d9ef">with</span> tqdm(train_loader, desc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Train&#34;</span>) <span style="color:#66d9ef">as</span> tbatch:
        <span style="color:#66d9ef">for</span> i, (samples, targets) <span style="color:#f92672">in</span> enumerate(tbatch):
            model<span style="color:#f92672">.</span>train()
            samples <span style="color:#f92672">=</span> samples<span style="color:#f92672">.</span>to(device)<span style="color:#f92672">.</span>long()
            targets <span style="color:#f92672">=</span> targets<span style="color:#f92672">.</span>to(device)
            model<span style="color:#f92672">.</span>zero_grad()
            predictions, _ <span style="color:#f92672">=</span> model(samples<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>))
            loss <span style="color:#f92672">=</span> criterion(predictions<span style="color:#f92672">.</span>squeeze(), targets<span style="color:#f92672">.</span>float())
            acc <span style="color:#f92672">=</span> (predictions<span style="color:#f92672">.</span>round()<span style="color:#f92672">.</span>squeeze() <span style="color:#f92672">==</span> targets)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
            acc <span style="color:#f92672">=</span> acc <span style="color:#f92672">/</span> batch_size
            loss<span style="color:#f92672">.</span>backward()
            torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">5</span>)
            optimizer<span style="color:#f92672">.</span>step()

            batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>item())
            batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]<span style="color:#f92672">.</span>append(acc)
            
            tbatch<span style="color:#f92672">.</span>set_postfix(loss<span style="color:#f92672">=</span>sum(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]),
                               acc<span style="color:#f92672">=</span>sum(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]))</code></pre></div>
</div>
<p>Among things that Keras does implicitly are gradient clipping and averaging of the loss and metrics. But there is also a detail which is the order of dimensions. Keras by default consider the batch size to be the first dimension of an input tensor. In PyTorch, for sequential data (this is our case, we effectively have input data in the form of a sequence of word tokens) the order with the sequence length first is preferred. That is why we need to transpose the data before feeding it to the network.</p>
<p>
We need to evaluate the model on each epoch end.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">epoch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]<span style="color:#f92672">.</span>append(sum(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;loss&#34;</span>]))
epoch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]<span style="color:#f92672">.</span>append(sum(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]) <span style="color:#f92672">/</span> len(batch_history[<span style="color:#e6db74">&#34;accuracy&#34;</span>]))
model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Validation...&#34;</span>)
val_loss, val_accuracy <span style="color:#f92672">=</span> validate(model, valid_loader)
epoch_history[<span style="color:#e6db74">&#34;val_loss&#34;</span>]<span style="color:#f92672">.</span>append(val_loss)
epoch_history[<span style="color:#e6db74">&#34;val_accuracy&#34;</span>]<span style="color:#f92672">.</span>append(val_accuracy)
<span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;{epoch_history=}&#34;</span>)</code></pre></div>
</div>
<pre class="example">
25000 Training sequences
25000 Validation sequences
</pre>
</div>
</div>
<div id="outline-container-headline-15" class="outline-3">
<h3 id="headline-15">
Results
</h3>
<div id="outline-text-headline-15" class="outline-text-3">
<p>After two epochs of training, we classification accuracy on the validation dataset gets over 80%. The loss and accuracy on the validation set after each epoch</p>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">Keras loss</th>
<th class="align-right">PyTorch loss</th>
<th class="align-right">Keras accuracy</th>
<th class="align-right">PyTorch accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.3639</td>
<td class="align-right">0.4024</td>
<td class="align-right">0.8440</td>
<td class="align-right">0.8233</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.3468</td>
<td class="align-right">0.3342</td>
<td class="align-right">0.8542</td>
<td class="align-right">0.8624</td>
</tr>
</tbody>
</table>
<p>
   The graph for the loss and accuracy during training.</p>
</div>
</div>
<div id="outline-container-headline-16" class="outline-3">
<h3 id="headline-16">
Conclusion
</h3>
<div id="outline-text-headline-16" class="outline-text-3">
<p>Reimplementing code is a hard task, since many things have to be taken into consideration. Parameter initialization, LSTM hidden states, average of metrics and losses, batch shuffling. Yet, we could obtain similar results which prove our success today! I hope this tutorial have introduced to you the differences between the two frameworks or maybe you rediscovered some concepts from a different viewpoint. We compared Keras and PyTorch, but there are a lot of other frameworks, with their advantages and specificities. I thank you for your attention and hope you the best in your machine learning journey!</p>
</div>
</div>
<div id="outline-container-headline-17" class="outline-3">
<h3 id="headline-17">
References
</h3>
<div id="outline-text-headline-17" class="outline-text-3">
<ul>
<li>
<p><a href="https://www.kaggle.com/arunmohan003/sentiment-analysis-using-lstm-pytorch">https://www.kaggle.com/arunmohan003/sentiment-analysis-using-lstm-pytorch</a></p>
</li>
<li>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-18" class="outline-2">
<h2 id="headline-18">
Example: Sentiment analysis in Keras and PyTorch
</h2>
<div id="outline-text-headline-18" class="outline-text-2">
<div id="outline-container-headline-19" class="outline-3">
<h3 id="headline-19">
BiLSTM in Keras
</h3>
<div id="outline-text-headline-19" class="outline-text-3">
<p>From Keras official tutorial<sup class="footnote-reference"><a id="footnote-reference-6" href="#footnote-6">6</a></sup>.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Bidirectional, LSTM, Dense, Embedding
<span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential

model <span style="color:#f92672">=</span> Sequential([
 Embedding(input_dim, <span style="color:#ae81ff">128</span>),
 Bidirectional(LSTM(hidden_dim, return_sequences<span style="color:#f92672">=</span>True)),
 Bidirectional(LSTM(hidden_dim)),
 Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)
])
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
model<span style="color:#f92672">.</span>summary()

model<span style="color:#f92672">.</span>fit(x<span style="color:#f92672">=</span>x_train, y<span style="color:#f92672">=</span>y_train,
       batch_size<span style="color:#f92672">=</span>batch_size, epochs<span style="color:#f92672">=</span>epochs,
       validation_data<span style="color:#f92672">=</span>(x_val, y_val),
       shuffle<span style="color:#f92672">=</span>False)</code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-20" class="outline-3">
<h3 id="headline-20">
BiLSTM in PyTorch
</h3>
<div id="outline-text-headline-20" class="outline-text-3">
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTM</span>(nn<span style="color:#f92672">.</span>Module):
 <span style="color:#66d9ef">def</span> __init__(self, input_dim, embedding_dim, hidden_dim, num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
     super()<span style="color:#f92672">.</span>__init__()
     self<span style="color:#f92672">.</span>input_dim <span style="color:#f92672">=</span> input_dim
     self<span style="color:#f92672">.</span>embedding_dim <span style="color:#f92672">=</span> embedding_dim
     self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
     self<span style="color:#f92672">.</span>num_layers <span style="color:#f92672">=</span> num_layers
     self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(input_dim, embedding_dim)
     self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim,
                         num_layers<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>num_layers, bidirectional<span style="color:#f92672">=</span>True)
     self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
     self<span style="color:#f92672">.</span>activation <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sigmoid()
     initrange <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
     self<span style="color:#f92672">.</span>encoder<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>uniform_(<span style="color:#f92672">-</span>initrange, initrange)
     nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>weight)
     self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
 <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, src):
     batch_size <span style="color:#f92672">=</span> src<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(src)
     output, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(output)
     output <span style="color:#f92672">=</span> output[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output)
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(output)
     <span style="color:#66d9ef">return</span> output</code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-21" class="outline-3">
<h3 id="headline-21">
Adapt the dataset to Pytorch
</h3>
<div id="outline-text-headline-21" class="outline-text-3">
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x_train <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>LongTensor(x_train)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
y_train <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y_train)
x_val <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>LongTensor(x_val)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
y_val <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y_val)</code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-22" class="outline-3">
<h3 id="headline-22">
Keras and PyTorch
</h3>
<div id="outline-text-headline-22" class="outline-text-3">
<ul>
<li>
<p><code>model.summary()</code> in Keras gives</p>
<pre class="example">
Model: &#34;sequential&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 128)         2560000   
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         98816     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128)               98816     
_________________________________________________________________
dense (Dense)                (None, 1)                 129       
=================================================================
Total params: 2,757,761
Trainable params: 2,757,761
Non-trainable params: 0
_________________________________________________________________
</pre>
<ul>
<li>
<p><code>model</code> in PyTorch gives</p>
<pre class="example">
BiLSTM(
(encoder): Embedding(20000, 128)
(lstm): LSTM(128, 64, num_layers=2, bidirectional=True)
(linear): Linear(in_features=128, out_features=1, bias=True)
(activation): Sigmoid()
)
</pre>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-headline-23" class="outline-3">
<h3 id="headline-23">
Bugs in PyTorch
</h3>
<div id="outline-text-headline-23" class="outline-text-3">
<ul>
<li>
<p>If I do not use data loading with Dataset/dataloader classes in pytorch,  the model is unable to learn</p>
<ul>
<li>
<p>Keras</p>
</li>
</ul>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">loss</th>
<th class="align-right">accuracy</th>
<th class="align-right">val_loss</th>
<th class="align-right">val_accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.4967</td>
<td class="align-right">0.7560</td>
<td class="align-right">0.3636</td>
<td class="align-right">0.8464</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.2837</td>
<td class="align-right">0.8861</td>
<td class="align-right">0.3433</td>
<td class="align-right">0.8584</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>PyTorch</p>
</li>
</ul>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">loss</th>
<th class="align-right">train_accuracy</th>
<th class="align-right">eval_loss</th>
<th class="align-right">accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.0216</td>
<td class="align-right">1</td>
<td class="align-right">1.25</td>
<td class="align-right">0.656</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.0012</td>
<td class="align-right">1</td>
<td class="align-right">1.69</td>
<td class="align-right">0.656</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Using BCEWithLogitsLoss is a bad idea because it makes the model unable to learn. Sigmoid + BCELoss works</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-24" class="outline-2">
<h2 id="headline-24">
Pytorch training and evaluation mode
</h2>
<div id="outline-text-headline-24" class="outline-text-2">
<div id="outline-container-headline-25" class="outline-3">
<h3 id="headline-25">
Pytorch training and evaluation mode
</h3>
<div id="outline-text-headline-25" class="outline-text-3">
<ul>
<li>
<p><code>model.training</code> is a switch used by <code>model.train()</code> and <code>model.eval()</code> to change the behaviour of the model. Calling these functions will simply change <code>model.training</code> variable to <code>True</code> or <code>False</code>.</p>
</li>
</ul>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> CustomModel()
model<span style="color:#f92672">.</span>train() <span style="color:#75715e"># Put model in training model and return it</span>
<span style="color:#66d9ef">print</span>(model<span style="color:#f92672">.</span>training) <span style="color:#75715e"># Output: True</span>
model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">print</span>(model<span style="color:#f92672">.</span>training() <span style="color:#75715e"># Output: False</span></code></pre></div>
</div>
<ul>
<li>
<p>There are similar methods like <code>model.numpy()</code> to convert to numpy array, <code>model.to(device)</code> to send data or models to device, etc.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-26" class="outline-2">
<h2 id="headline-26">
Work with tensors
</h2>
<div id="outline-text-headline-26" class="outline-text-2">
<div id="outline-container-headline-27" class="outline-3">
<h3 id="headline-27">
.size and .shape
</h3>
<div id="outline-text-headline-27" class="outline-text-3">
<ul>
<li>
<p>Pytorch has .shape and .size which are both equivalent</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>))
 t<span style="color:#f92672">.</span>shape, t<span style="color:#f92672">.</span>size()         <span style="color:#75715e"># Both equal to (4, 3)</span>
 t<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], t<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)     <span style="color:#75715e"># Both equal to 3</span></code></pre></div>
</div>
</li>
<li>
<p>Tensorflow now supports <code>.shape</code> too, similar to numpy</p>
</li>
</ul>
</div>
</div>
<div id="outline-container-headline-28" class="outline-3">
<h3 id="headline-28">
Order of dimensions in Pytorch
</h3>
<div id="outline-text-headline-28" class="outline-text-3">
<ul>
<li>
<p>Keras usually orders dimensions <code>(batch_size, seq_len, input_dim)</code></p>
</li>
<li>
<p>Pytorch prefers to order them as <code>(seq_len, batch_size, input_dim)</code></p>
<ul>
<li>
<p>There is a switch <code>batch_first</code> in RNN, LSTM and GRU modules. But others like Transformer modules have no such parameter and you need to follow the default order.</p>
</li>
<li>
<p>You can switch dimensions in Pytorch using <code>.transpose</code> method.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(tensor_with_batch_first)
data<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)            <span style="color:#75715e"># Switch first and second dimensions</span></code></pre></div>
</div>
</li>
</ul>
</li>
<li>
<p>The order chosen by PyTorch is more natural for parallelizing computation, whereas Keras order is more natural in terms of architecture of models.</p>
</li>
</ul>
</div>
</div>
<div id="outline-container-headline-29" class="outline-3">
<h3 id="headline-29">
Initialize vectors in PyTorch
</h3>
<div id="outline-text-headline-29" class="outline-text-3">
<p>Very similar to Numpy.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  torch<span style="color:#f92672">.</span>ones(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>)         <span style="color:#75715e"># Matrix of size (2, 4, 1) filled with 1. Same with torch.zeros</span>
  torch<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">3</span>)                <span style="color:#75715e"># Identity matrix of size (3,3)</span>
  torch<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># Initialize the seed for RNG</span>
  torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>)            <span style="color:#75715e"># Sample from N(0, 1) a matrix of size (2, 3)</span>
  torch<span style="color:#f92672">.</span>full((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>), fill_value<span style="color:#f92672">=</span><span style="color:#ae81ff">3.14</span>)  <span style="color:#75715e"># Fill a (2, 4) matrix with 3.14 value.</span></code></pre></div>
</div>
<p><code>torch.full</code> is the equivalent of <code>numpy.fill</code></p>
<p>
  See documentation for more sampling<sup class="footnote-reference"><a id="footnote-reference-7" href="#footnote-7">7</a></sup></p>
</div>
</div>
<div id="outline-container-headline-30" class="outline-3">
<h3 id="headline-30">
Random sampling from a probability distribution
</h3>
<div id="outline-text-headline-30" class="outline-text-3">
<p>You can sample from a probability distribution vector</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> probability_distribution <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor([<span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.4</span>])
 <span style="color:#66d9ef">print</span>(probability_distribution<span style="color:#f92672">.</span>sum())  <span style="color:#75715e"># = 1</span>
 multinomial_sample <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probability_distribution)

 poisson_parameters <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn((<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)) <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>
 poisson_sample <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>poisson(poisson_distribution)

 event_probabilities <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">0.7</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.9</span>]
 bernoulli_sample <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bernoulli(event_probabilities)</code></pre></div>
</div>
<p>For example
   \[
   \text{out}_i \sim \text{Poisson}(\text{input}_i)
   \]
   \[
   \text{out}_i \sim \text{Bernoulli}(p = \text{input}_i)
   \]</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-31" class="outline-2">
<h2 id="headline-31">
Use GPU
</h2>
<div id="outline-text-headline-31" class="outline-text-2">
<div id="outline-container-headline-32" class="outline-3">
<h3 id="headline-32">
Use GPU (PyTorch)
</h3>
<div id="outline-text-headline-32" class="outline-text-3">
<ul>
<li>
<p>In PyTorch simply do</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(
      <span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)</code></pre></div>
</div>
</li>
<li>
<p>Usage</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"> model <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>GRU(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>)
 data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">5</span>)
 model<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># Send model parameters to device</span>
 data<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># Send tensor to device</span></code></pre></div>
</div>
</li>
<li>
<p>You can check the device on which a model or a tensor is with <code>model.device</code> or <code>data.device</code>.</p>
</li>
</ul>
</div>
</div>
<div id="outline-container-headline-33" class="outline-3">
<h3 id="headline-33">
Use GPU (Keras)
</h3>
<div id="outline-text-headline-33" class="outline-text-3">
<p>To check if the GPU is available for Keras, use the following</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  tensorflow<span style="color:#f92672">.</span>test<span style="color:#f92672">.</span>is_gpu_available()</code></pre></div>
</div>
</div>
</div>
<div id="outline-container-headline-34" class="outline-3">
<h3 id="headline-34">
Parallelization (Keras)
</h3>
<div id="outline-text-headline-34" class="outline-text-3">
<p>To show the number of GPUs:</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Num GPUs Available: &#34;</span>, len(tf<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>experimental<span style="color:#f92672">.</span>list_physical_devices(<span style="color:#e6db74">&#39;GPU&#39;</span>)))</code></pre></div>
</div>
<p><a href="https://www.tensorflow.org/guide/distributed_training">https://www.tensorflow.org/guide/distributed_training</a>
<a href="https://keras.io/getting_started/faq/#how-can-i-train-a-keras-model-on-multiple-gpus-on-a-single-machine">https://keras.io/getting_started/faq/#how-can-i-train-a-keras-model-on-multiple-gpus-on-a-single-machine</a> <a href="https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</a></p>
</div>
</div>
<div id="outline-container-headline-35" class="outline-3">
<h3 id="headline-35">
COMMENT Performances
</h3>
</div>
</div>
</div>
<div id="outline-container-headline-36" class="outline-2">
<h2 id="headline-36">
Conclusion
</h2>
<div id="outline-text-headline-36" class="outline-text-2">
<div id="outline-container-headline-37" class="outline-3">
<h3 id="headline-37">
Keras objects and PyTorch objects
</h3>
<div id="outline-text-headline-37" class="outline-text-3">
<table>
<thead>
<tr>
<th></th>
<th>Keras</th>
<th>Pytorch</th>
</tr>
</thead>
<tbody>
<tr>
<td>naming</td>
<td>Model.call</td>
<td>Module.forward</td>
</tr>
<tr>
<td></td>
<td>layers.Layer</td>
<td>nn.Module</td>
</tr>
<tr>
<td></td>
<td>layers.Dense</td>
<td>nn.Linear (without activation)</td>
</tr>
<tr>
<td></td>
<td>layers.LSTM(return_sequences=True)</td>
<td>nn.LSTM</td>
</tr>
<tr>
<td></td>
<td>utils.Sequence</td>
<td>utils.data.Dataset</td>
</tr>
</tbody>
<tbody>
<tr>
<td>Use</td>
<td>Fast prototyping</td>
<td>Custom implementations, processing, training</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-headline-38" class="outline-3">
<h3 id="headline-38">
Conclusion
</h3>
<div id="outline-text-headline-38" class="outline-text-3">
<ul>
<li>
<p>Pytorch syntax is closer to Python</p>
<ul>
<li>
<p><code>model.to()</code>, <code>data.numpy()</code>, <code>model.train()</code> are very handy</p>
</li>
</ul>
</li>
<li>
<p>Keras and Pytorch are very similar for data loading</p>
</li>
<li>
<p>Keras is best for fast prototyping</p>
<ul>
<li>
<p>But Pytorch has a similar library called Pytorch lightning</p>
</li>
</ul>
</li>
<li>
<p>Pytorch has a great documentation<sup class="footnote-reference"><a id="footnote-reference-8" href="#footnote-8">8</a></sup></p>
</li>
<li>
<p>Detailed comparison of Keras and PyTorch which displays a simple example of ConvNet architecture: <a href="https://deepsense.ai/keras-or-pytorch/">https://deepsense.ai/keras-or-pytorch/</a></p>
</li>
<li>
<p>In terms of speed, Keras is much slower, while PyTorch and Tensorflow have similar results. Old version of PyTorch (0.2.0) <a href="https://wrosinski.github.io/deep-learning-frameworks/">https://wrosinski.github.io/deep-learning-frameworks/</a></p>
</li>
<li>
<p>From Ilia Karmanov&#39;s benchmark, for multi-GPU, PyTorch and Keras are straightforward, Tensorflow is more tedious. <a href="https://medium.com/@iliakarmanov/multi-gpu-rosetta-stone-d4fa96162986">https://medium.com/@iliakarmanov/multi-gpu-rosetta-stone-d4fa96162986</a></p>
</li>
<li>
<p>Ilia Karmanov has a git repository of codes adapted from Keras to PyTorch and vice versa <a href="https://github.com/ilkarman/DeepLearningFrameworks/">https://github.com/ilkarman/DeepLearningFrameworks/</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-39" class="outline-2">
<h2 id="headline-39">
Sentiment classification in Keras and PyTorch
</h2>
<div id="outline-text-headline-39" class="outline-text-2">
<p>Now that we have some insight on the two different syntaxes, let&#39;s implement the same architecture in both frameworks. There are functions available in both Keras and PyTorch to load popular annotated datasets such as MNIST, IMDB, etc. We will use IMDB dataset which is a dataset of movie reviews, annotated by sentiment polarity, whether negative or positive. We will use Keras to load the data for both models.</p>
<p>
  The architecture we are going to use is a two layers bidirectional-LSTM with an embedding layer. I took a <a href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/">code example from Keras official tutorial</a> and then implemented the same into PyTorch.</p>
<p>
  We commence by loading the IMDB move review dataset.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.datasets <span style="color:#f92672">import</span> imdb
<span style="color:#f92672">from</span> tensorflow.keras.preprocessing.sequence <span style="color:#f92672">import</span> pad_sequences

input_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">20000</span>
embedding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
hidden_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>
epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>
maxlen <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>

(x_train, y_train), (x_val, y_val) <span style="color:#f92672">=</span> imdb<span style="color:#f92672">.</span>load_data(num_words<span style="color:#f92672">=</span>input_dim)
x_train <span style="color:#f92672">=</span> pad_sequences(x_train, maxlen<span style="color:#f92672">=</span>maxlen)
x_val <span style="color:#f92672">=</span> pad_sequences(x_val, maxlen<span style="color:#f92672">=</span>maxlen)
<span style="color:#66d9ef">print</span>(len(x_train), <span style="color:#e6db74">&#34;Training sequences&#34;</span>)
<span style="color:#66d9ef">print</span>(len(x_val), <span style="color:#e6db74">&#34;Validation sequences&#34;</span>)</code></pre></div>
</div>
<pre class="example">
  25000 Training sequences
  25000 Validation sequences
</pre>
<div id="outline-container-headline-40" class="outline-3">
<h3 id="headline-40">
Model and training in Keras
</h3>
<div id="outline-text-headline-40" class="outline-text-3">
<p>We have the following code, which defines the model and trains the network.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Bidirectional, LSTM, Dense, Embedding
<span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential

model <span style="color:#f92672">=</span> Sequential([
 Embedding(input_dim, <span style="color:#ae81ff">128</span>),
 Bidirectional(LSTM(hidden_dim, return_sequences<span style="color:#f92672">=</span>True)),
 Bidirectional(LSTM(hidden_dim)),
 Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)
])
model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
model<span style="color:#f92672">.</span>summary()

model<span style="color:#f92672">.</span>fit(x<span style="color:#f92672">=</span>x_train, y<span style="color:#f92672">=</span>y_train,
       batch_size<span style="color:#f92672">=</span>batch_size, epochs<span style="color:#f92672">=</span>epochs,
       validation_data<span style="color:#f92672">=</span>(x_val, y_val),
       shuffle<span style="color:#f92672">=</span>False)</code></pre></div>
</div>
<p>Here is the output of training.</p>
<pre class="example">
Epoch 1/2
782/782 [==============================] - 409s 524ms/step - loss: 0.4074 - accuracy: 0.8139 - val_loss: 0.3385 - val_accuracy: 0.8602
Epoch 2/2
782/782 [==============================] - 394s 504ms/step - loss: 0.2756 - accuracy: 0.8909 - val_loss: 0.3428 - val_accuracy: 0.8517
</pre>
</div>
</div>
<div id="outline-container-headline-41" class="outline-3">
<h3 id="headline-41">
Model an training in PyTorch
</h3>
<div id="outline-text-headline-41" class="outline-text-3">
<p>In PyTorch we define the architecture as a module.
   Though I did not find any trace of gradient clipping techniques applied during training for Keras, I noticed gradient tend to be unstable and diverge in PyTorch without it. So I assumed a default value of 1.0, which gives sensibly close results to Keras. </p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BiLSTM</span>(nn<span style="color:#f92672">.</span>Module):
 <span style="color:#66d9ef">def</span> __init__(self, input_dim, embedding_dim, hidden_dim, num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
     super()<span style="color:#f92672">.</span>__init__()
     self<span style="color:#f92672">.</span>input_dim <span style="color:#f92672">=</span> input_dim
     self<span style="color:#f92672">.</span>embedding_dim <span style="color:#f92672">=</span> embedding_dim
     self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
     self<span style="color:#f92672">.</span>num_layers <span style="color:#f92672">=</span> num_layers
     self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(input_dim, embedding_dim)
     self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim,
                         num_layers<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>num_layers, bidirectional<span style="color:#f92672">=</span>True)
     self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
     initrange <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>
     self<span style="color:#f92672">.</span>encoder<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>uniform_(<span style="color:#f92672">-</span>initrange, initrange)
     nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>weight)
     self<span style="color:#f92672">.</span>linear<span style="color:#f92672">.</span>bias<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>zero_()
 <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, src):
     batch_size <span style="color:#f92672">=</span> src<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(src)
     output, _ <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(output)
     output <span style="color:#f92672">=</span> output[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
     output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(output)
     <span style="color:#66d9ef">return</span> output</code></pre></div>
</div>
<p>The forward function is the same as before. Then how did we define the two layers bidirectional LSTM? It is in this statement in the <code>__init__</code></p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">  self<span style="color:#f92672">.</span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(embedding_dim, hidden_dim,
                   num_layers<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>num_layers, bidirectional<span style="color:#f92672">=</span>True)</code></pre></div>
</div>
<p>By setting <code>num_layers=2</code> and <code>bidirectional=True</code>.</p>
<p>
   Ok let&#39;s write the code for training this model now! We have to convert the loaded dataset into Torch tensors.</p>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x_train <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>LongTensor(x_train)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
y_train <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y_train)
x_val <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>LongTensor(x_val)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
y_val <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(y_val)</code></pre></div>
</div>
<p>Output of training (PyTorch)</p>
<pre class="example">
100%|===========| 781/781 [01:02&lt;00:00, 12.50batch/s, acc=0.656, 
                        eval_loss=1.25, loss=0.0216, train_acc=1]
100%|===========| 781/781 [01:02&lt;00:00, 12.46batch/s, acc=0.656, 
                        eval_loss=1.69, loss=0.0012, train_acc=1]
Train: 100%|==| 2/2 [02:05&lt;00:00, 62.57s/epoch]
</pre>
</div>
</div>
<div id="outline-container-headline-42" class="outline-3">
<h3 id="headline-42">
Pytorch training and evaluation mode
</h3>
<div id="outline-text-headline-42" class="outline-text-3">
<ul>
<li>
<p><code>model.training</code> is a switch used by <code>model.train()</code> and <code>model.eval()</code> to change the behaviour of the model. Calling these functions will simply change <code>model.training</code> variable to <code>True</code> or <code>False</code>.</p>
</li>
</ul>
<div class="src src-python">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> CustomModel()
model<span style="color:#f92672">.</span>train() <span style="color:#75715e"># Put model in training model and return it</span>
<span style="color:#66d9ef">print</span>(model<span style="color:#f92672">.</span>training) <span style="color:#75715e"># Output: True</span>
model<span style="color:#f92672">.</span>eval()
<span style="color:#66d9ef">print</span>(model<span style="color:#f92672">.</span>training() <span style="color:#75715e"># Output: False</span></code></pre></div>
</div>
<ul>
<li>
<p>There are similar methods like <code>model.numpy()</code> to convert to numpy array, <code>model.to(device)</code> to send data or models to device, etc.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-43" class="outline-2">
<h2 id="headline-43">
Comparison of results
</h2>
<div id="outline-text-headline-43" class="outline-text-2">
<p>If we inspect the model architecture, we have the following.
  In Keras, with <code>model.summary()</code></p>
<pre class="example">
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 128)         2560000   
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         98816     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128)               98816     
_________________________________________________________________
dense (Dense)                (None, 1)                 129       
=================================================================
Total params: 2,757,761
Trainable params: 2,757,761
Non-trainable params: 0
_________________________________________________________________
Epoch 1/2
782/782 [==============================] - 409s 524ms/step - loss: 0.4074 - accuracy: 0.8139 - val_loss: 0.3385 - val_accuracy: 0.8602
Epoch 2/2
782/782 [==============================] - 394s 504ms/step - loss: 0.2756 - accuracy: 0.8909 - val_loss: 0.3428 - val_accuracy: 0.8517
</pre>
<p>In PyTorch, with simply <code>print(model)</code>.</p>
<pre class="example">
BiLSTM(
(encoder): Embedding(20000, 128)
(lstm): LSTM(128, 64, num_layers=2, bidirectional=True)
(linear): Linear(in_features=128, out_features=1, bias=True)
)
</pre>
<p>The results of above experiments are different between the two setups.</p>
<ul>
<li>
<p>Training</p>
</li>
</ul>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">loss (Keras)</th>
<th class="align-right">loss (PyTorch)</th>
<th class="align-right">accuracy (Keras)</th>
<th class="align-right">accuracy (PyTorch)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.4074</td>
<td class="align-right">0.507</td>
<td class="align-right">0.8139</td>
<td class="align-right">0.73</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.2756</td>
<td class="align-right">0.318</td>
<td class="align-right">0.8909</td>
<td class="align-right">0.846</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>Validation</p>
</li>
</ul>
<table>
<thead>
<tr>
<th class="align-right">Epoch</th>
<th class="align-right">val_loss (Keras)</th>
<th class="align-right">val_loss (PyTorch)</th>
<th class="align-right">val_accuracy (Keras)</th>
<th class="align-right">val_accuracy (PyTorch)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="align-right">1</td>
<td class="align-right">0.3385</td>
<td class="align-right">0.5017</td>
<td class="align-right">0.8602</td>
<td class="align-right">0.8291</td>
</tr>
<tr>
<td class="align-right">2</td>
<td class="align-right">0.3428</td>
<td class="align-right">1.69</td>
<td class="align-right">0.8517</td>
<td class="align-right">0.656</td>
</tr>
</tbody>
</table>
<p>It should be noted that even with great care, it is very hard to obtain same results for a sensibly identical architecture, data processing and training, because each framework will have its own internal mechanisms and designs, with tricks for numerical stability which won&#39;t give the exact same results. </p>
</div>
</div>
<div id="outline-container-headline-44" class="outline-2">
<h2 id="headline-44">
Refs
</h2>
</div>
<div class="footnotes">
<hr class="footnotes-separatator">
<div class="footnote-definitions">
</div>
</div>

  </div>
  <footer>
    <ul class="stats">
  
    
    
      <li class="categories">
        <ul>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/categories/machine-learning">Machine Learning</a></li>
          
        </ul>
      </li>
    
  
  
    
    
      <li class="tags">
        <ul>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/tags/machine-learning">machine-learning</a></li>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/tags/pytorch">pytorch</a></li>
          
            
            <li><a class="article-category-link" href="https://adamoudad.github.io/tags/keras">keras</a></li>
          
        </ul>
      </li>
    
  
</ul>

  </footer>
</article>

    <article class="post">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "adamoudad-github-io" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </article>




<div class="pagination">
  
    <a href="/posts/lock_pattern/" class="button"><div class="previous"><div>How many patterns are there to lock your android smartphone?</div></div></a>
  
  
    <a href="/posts/keras_pytorch_comparison/" class="button"><div class="next"><div>Comparison of Keras and PyTorch syntaxes</div></div></a>
  
</div>


      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent posts</h1>
      </header>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/keras_pytorch_comparison/">Comparison of Keras and PyTorch syntaxes</a></h1>
          <time class="published" datetime="">March 2, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/keras_torch_comparison/">Guide to swtich between Keras and PyTorch</a></h1>
          <time class="published" datetime="">January 30, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/lock_pattern/">How many patterns are there to lock your android smartphone?</a></h1>
          <time class="published" datetime="">January 16, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/pierre-corneille-bot/">Ask this bot to solve your dilemmas</a></h1>
          <time class="published" datetime="">November 18, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
        <section>
          

        </section>
        <header>
          <h1><a href="/posts/progress_bar_with_tqdm/">Training models with a progress bar</a></h1>
          <time class="published" datetime="">October 12, 2020</time>
        </header>
      </article>
      
      
        <a href="/posts/" class="button">See more</a>
      
    </section>
  

  
    
      <section id="categories">
        <header>
          <h1><a href="/categories">Categories</a></h1>
        </header>
        <ul>
          
            
          
          
          <li>
            
              <a href="/categories/machine-learning/">machine-learning<span class="count">9</span></a>
            
          
          <li>
            
              <a href="/categories/programming/">programming<span class="count">6</span></a>
            
          
          <li>
            
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">3</span></a>
            
          
          <li>
            
              <a href="/categories/japanese/">japanese<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/linux/">linux<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/python/">python<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/security/">security<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/web/">web<span class="count">2</span></a>
            
          
          <li>
            
              <a href="/categories/emacs/">emacs<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/linguistic/">linguistic<span class="count">1</span></a>
            
          
          <li>
            
              <a href="/categories/random/">random<span class="count">1</span></a>
            
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>This website is a weblog were I write about computer science, machine learning, language learning.</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/adamoudad" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>

<li><a href="//stackoverflow.com/users/7013114/adam-oudad" target="_blank" rel="noopener" title="Stack Overflow" class="fab fa-stack-overflow"></a></li>








<li><a href="//medium.com/@adam.oudad" target="_blank" rel="noopener" title="Medium" class="fab fa-medium"></a></li>
<li><a href="//linkedin.com/in/adam-oudad-9436b866" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>















<li><a href="//twitter.com/OudadAdam" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>







<li><a href="//researchgate.net/profile/Adam_Oudad" target="_blank" rel="noopener" title="Research Gate"><i class="ai ai-researchgate"></i></a></li>





      </ul>
  
  <p class="copyright">
    
      &copy; 2021
      
        Adam Oudad
      
    . <br>
    Theme: <a href='https://github.com/pacollins/hugo-future-imperfect-slim' target='_blank' rel='noopener'>Hugo Future Imperfect Slim</a><br>A <a href='https://html5up.net/future-imperfect' target='_blank' rel='noopener'>HTML5 UP port</a> | Powered by <a href='https://gohugo.io/' title='0.80.0' target='_blank' rel='noopener'>Hugo</a>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


      <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="/js/bundle.min.e0c399b948d3cf5c3a5e3badb94023d0b064a19ac39fd8cd79fb9f57fa4d1091.js" integrity="sha256-4MOZuUjTz1w6XjutuUAj0LBkoZrDn9jNefufV/pNEJE="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150494000-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>
