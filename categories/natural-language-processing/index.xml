<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on Adam Oudad</title>
    <link>https://adamoudad.github.io/categories/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on Adam Oudad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2019 03:49:04 +0900</lastBuildDate>
    
	<atom:link href="https://adamoudad.github.io/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Vectorizing text</title>
      <link>https://adamoudad.github.io/post/vectorizing-text/</link>
      <pubDate>Thu, 17 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/vectorizing-text/</guid>
      <description>Character, word, sentence and document embeddings are popular because they are efficient. In the case of words, such embeddings represent words in their meaning, their role and their hierarchy in texts.  We have seen a clear improvement of word embeddings thanks to Mikolov et al. at Google, with Word2Vec in 2014. Since then, many kinds of embeddings have been developped for different purposes. We can cite for example Fasttext by Facebook, Bert by Google.</description>
    </item>
    
  </channel>
</rss>