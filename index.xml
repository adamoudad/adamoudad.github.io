<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adam Oudad</title>
    <link>https://adamoudad.github.io/</link>
    <description>Recent content on Adam Oudad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2019 03:49:04 +0900</lastBuildDate>
    
	<atom:link href="https://adamoudad.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Vectorizing text: Word2Vec</title>
      <link>https://adamoudad.github.io/post/word2vec/</link>
      <pubDate>Wed, 27 Nov 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/word2vec/</guid>
      <description>The idea with greatest impact in Word2Vec1 2 was that vector representations could capture linguistic regularities, which can be retrieved through vector arithmetics vec(&#39;Rome&#39;) ≈ vec(&#39;Paris&#39;) - vec(&#39;France&#39;) + vec(&#39;Italy&#39;) vec(&#39;Queen&#39;) ≈ vec(&#39;King&#39;) - vec(&#39;Man&#39;) + vec(&#39;Woman&#39;)   Awesome ! How could such results be achieved ? They came from the following assumption   The meaning of a word can be inferred by the company it keeps   Indeed, Word2Vec was built using unsupervised learning on huge quantity of text, by predicting words, given their context.</description>
    </item>
    
    <item>
      <title>List of datasets</title>
      <link>https://adamoudad.github.io/post/datasets/</link>
      <pubDate>Wed, 30 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/datasets/</guid>
      <description>Machine Learning requires data. To obtain some, one way is to scrape or retrieve data directly from somewhere. Another way is to reuse datasets freely available over the internet, specially made for most common tasks. Because it has become so frequent for data scientists to take much time searching and processing data, Google has its own Dataset search tool. There are also big lists of datasets for research purpose, such as this one.</description>
    </item>
    
    <item>
      <title>Usages of する</title>
      <link>https://adamoudad.github.io/post/japanese/suru/</link>
      <pubDate>Wed, 23 Oct 2019 02:31:19 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/japanese/suru/</guid>
      <description>Do you know する? This verb you can use in many case to say &amp;#34;I do stuff&amp;#34;, right ? Yet what if I told you could say far more than just a lame 私はスポーツする? する basics   We start here with the basics, that is the ~をする form. Use it to express an activity you do regularily. ラグビーをする。  ~がする   You can express a feeling, a sensation with ~がする.</description>
    </item>
    
    <item>
      <title>Word embeddings with sound</title>
      <link>https://adamoudad.github.io/post/sound-word2vec/</link>
      <pubDate>Mon, 21 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/sound-word2vec/</guid>
      <description>I came across this article called Sound-Word2Vec:Learning Word Representations Grounded in Sounds1, which caught my attention as it aims at creating word embeddings in an original way, using voiced sounds of words. What is embedding   Embedding in Machine Learning refers to a method for capturing the information of an input data into a dense representation. This way, we obtain an embedding space, which should have interesting properties like being friendly to vector arithmetics, which is not the case of original raw data.</description>
    </item>
    
    <item>
      <title>Vectorizing text</title>
      <link>https://adamoudad.github.io/post/vectorizing-text/</link>
      <pubDate>Thu, 17 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/vectorizing-text/</guid>
      <description>Character, word, sentence and document embeddings are popular because they are efficient. In the case of words, such embeddings represent words in their meaning, their role and their hierarchy in texts.  We have seen a clear improvement of word embeddings thanks to Mikolov et al. at Google, with Word2Vec in 2013. Since then, many kinds of embeddings have been developped for different purposes. We can cite for example Fasttext by Facebook, Bert by Google.</description>
    </item>
    
    <item>
      <title>When to use が or は?</title>
      <link>https://adamoudad.github.io/post/japanese/ga-or-ha/</link>
      <pubDate>Thu, 17 Oct 2019 02:31:19 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/japanese/ga-or-ha/</guid>
      <description>This is one of the most fearful grammatical points of any japanese learner during their journey to japanese mastery.  This article gives some rules that will help confusion and will make you sound like a native. Relatively to information intended to be delivered 伝えたい情報   は will always precede the information, whereas が will follow it. Similarily, は will always precede an interrogative, while が will follow it.</description>
    </item>
    
    <item>
      <title>Multiply your productivity by one million with these simple tricks</title>
      <link>https://adamoudad.github.io/post/productivity/</link>
      <pubDate>Thu, 10 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/productivity/</guid>
      <description>The idea behind most productivity tips is that you lose if you touch your mouse, as it is such a time consuming move and unbearable physical effort.  But you also lose points when doing big jumps on your keyboard, for example when you try reaching for the arrow keys.  So what should we do ? Here are some tips. Tridactyl on Firefox   Tridactyl is an extension available for Firefox that enable a keyboard-based navigation.</description>
    </item>
    
    <item>
      <title>Hugo and Katex</title>
      <link>https://adamoudad.github.io/post/hugo-katex/</link>
      <pubDate>Mon, 20 May 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/hugo-katex/</guid>
      <description>I followed this good tutorial on how to add \(\KaTeX\) to hugo.  Here is an example showing that KaTeX (should) now work. $$∫ab x^2 dx$$  Here are some remarks I have about the original tutorial. Auto-rendering   Add the following script in the footer &amp;lt;script defer src=&amp;#34;https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js&amp;#34; integrity=&amp;#34;sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF&amp;#34; crossorigin=&amp;#34;anonymous&amp;#34; onload=&amp;#34;renderMathInElement(document.body);&amp;#34;&amp;gt;&amp;lt;/script&amp;gt;   This will activate the auto rendering Other remarks     {{ end }} is unnecessary, only write the following to include the katex.</description>
    </item>
    
    <item>
      <title>First post</title>
      <link>https://adamoudad.github.io/post/my-first-post/</link>
      <pubDate>Thu, 16 May 2019 02:31:19 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/my-first-post/</guid>
      <description> This is the first post.  I use a theme called m10c created by Vaga, thanks to him !  I was using Jekyll in previous websites, and Hugo is quite impressive in ease of use.    Great themes    Many shortcodes that lets you embed youtube videos, for example, without coding   </description>
    </item>
    
  </channel>
</rss>