<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nlp on Adam Oudad</title>
    <link>https://adamoudad.github.io/tags/nlp/</link>
    <description>Recent content in nlp on Adam Oudad</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2019 03:49:04 +0900</lastBuildDate>
    
	<atom:link href="https://adamoudad.github.io/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Word embeddings with sound</title>
      <link>https://adamoudad.github.io/post/sound-word2vec/</link>
      <pubDate>Mon, 21 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/sound-word2vec/</guid>
      <description>I came across this article called Sound-Word2Vec:Learning Word Representations Grounded in Sounds1, which caught my attention as it aims at creating word embeddings in an original way, using voiced sounds of words. What is embedding   Embedding in Machine Learning refers to a method for capturing the information of an input data into a dense representation. This way, we obtain an embedding space, which should have interesting properties like being friendly to vector arithmetics, which is not the case of original raw data.</description>
    </item>
    
    <item>
      <title>Vectorizing text</title>
      <link>https://adamoudad.github.io/post/vectorizing-text/</link>
      <pubDate>Thu, 17 Oct 2019 03:49:04 +0900</pubDate>
      
      <guid>https://adamoudad.github.io/post/vectorizing-text/</guid>
      <description>Character, word, sentence and document embeddings are popular because they are efficient. In the case of words, such embeddings represent words in their meaning, their role and their hierarchy in texts.  We have seen a clear improvement of word embeddings thanks to Mikolov et al. at Google, with Word2Vec in 2013. Since then, many kinds of embeddings have been developped for different purposes. We can cite for example Fasttext by Facebook, Bert by Google.</description>
    </item>
    
  </channel>
</rss>